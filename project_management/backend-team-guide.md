# åç«¯å¼€å‘ä¸AIæ¨¡å‹å›¢é˜Ÿä»»åŠ¡æŒ‡å—

> **å›¢é˜Ÿè§„æ¨¡**: 5äºº (åŸåç«¯3äºº + åŸæ¨¡å‹2äºº)
> **æ ¸å¿ƒèŒè´£**: æ­å»ºæœåŠ¡æ¶æ„ã€å®ç°æ··åˆæ£€ç´¢ç®—æ³•ã€çŸ¥è¯†åº“ç®¡ç†ã€APIå¼€å‘ã€AIæ¨¡å‹é›†æˆã€RAGæµç¨‹ä¼˜åŒ–
> **æŠ€æœ¯æ ˆ**: Python + Flaskã€MySQLã€SQLAlchemyã€RQã€Dockerã€JWTè®¤è¯ã€é˜¿é‡Œäº‘æ–‡æ¡£æ™ºèƒ½ã€langgraphã€Weaviateã€Qwen3ç³»åˆ—æ¨¡å‹ã€OLLAMA

## ğŸ“‹ ä»»åŠ¡æ¦‚è§ˆ

### ä¸»è¦ä»»åŠ¡æ¸…å•

#### ğŸ—ï¸ åŸºç¡€æ¶æ„ä»»åŠ¡ (åç«¯ä¸»å¯¼)
- [x] **ä»»åŠ¡1**: Flaské¡¹ç›®æ­å»ºä¸è®¤è¯ç³»ç»Ÿ (é¢„è®¡2-3å¤©)
- [ ] **ä»»åŠ¡2**: æ•°æ®åº“è®¾è®¡ä¸æ¨¡å‹å®šä¹‰ (é¢„è®¡2-3å¤©)
- [ ] **ä»»åŠ¡3**: æ ¸å¿ƒä¸šåŠ¡APIå®ç° (é¢„è®¡5-6å¤©)
- [ ] **ä»»åŠ¡4**: çŸ¥è¯†æ–‡æ¡£ç®¡ç†API (é¢„è®¡3-4å¤©)
- [ ] **ä»»åŠ¡5**: å¼‚æ­¥ä»»åŠ¡å¤„ç†ç³»ç»Ÿ (é¢„è®¡3-4å¤©)
- [ ] **ä»»åŠ¡6**: æ•°æ®çœ‹æ¿APIå®ç° (é¢„è®¡2-3å¤©)

#### ğŸ¤– AIèƒ½åŠ›ä»»åŠ¡ (æ¨¡å‹ä¸»å¯¼)
- [ ] **ä»»åŠ¡7**: æ•°æ®æ”¶é›†ä¸IDPé›†æˆ (é¢„è®¡3-4å¤©)
- [ ] **ä»»åŠ¡8**: å‘é‡æ•°æ®åº“æ„å»ºä¸é…ç½® (é¢„è®¡2-3å¤©)
- [ ] **ä»»åŠ¡9**: æ··åˆæ£€ç´¢ç®—æ³•å®ç° (é¢„è®¡4-5å¤©)
- [ ] **ä»»åŠ¡10**: æç¤ºè¯å·¥ç¨‹ä¸ä¼˜åŒ– (é¢„è®¡3-4å¤©)
- [ ] **ä»»åŠ¡11**: langgraph Agentæµç¨‹æ„å»º (é¢„è®¡5-6å¤©)
- [ ] **ä»»åŠ¡12**: æ¨¡å‹æ¥å£é›†æˆä¸æµ‹è¯• (é¢„è®¡2-3å¤©)

#### ğŸ”§ é›†æˆæµ‹è¯•ä»»åŠ¡ (å…¨å›¢é˜Ÿ)
- [ ] **ä»»åŠ¡13**: ç³»ç»Ÿé›†æˆä¸ç«¯åˆ°ç«¯æµ‹è¯• (é¢„è®¡3-4å¤©)
- [ ] **ä»»åŠ¡14**: (è¿›é˜¶)å°æ¨¡å‹å¾®è°ƒ (é¢„è®¡5-7å¤©ï¼Œå¯é€‰)

---

## ğŸ¯ ä»»åŠ¡1: Flaské¡¹ç›®æ­å»ºä¸è®¤è¯ç³»ç»Ÿ

### 1.1 é¡¹ç›®åˆå§‹åŒ– (ç¬¬1å¤©)

**ç›®æ ‡**: åˆ›å»ºFlaské¡¹ç›®åŸºç¡€ç»“æ„ï¼Œé…ç½®å¼€å‘ç¯å¢ƒã€‚

**å…·ä½“æ­¥éª¤**:

1. **åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„**
   ```bash
   mkdir ip-expert-backend
   cd ip-expert-backend
   
   # åˆ›å»ºç›®å½•ç»“æ„
   mkdir -p {app/{api,models,services,utils},config,migrations,tests}
   touch app/__init__.py app/api/__init__.py app/models/__init__.py
   ```

2. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   # æˆ– venv\Scripts\activate  # Windows
   ```

3. **å®‰è£…ä¾èµ–åŒ…** (`requirements.txt`)
   ```txt
   Flask==2.3.3
   Flask-SQLAlchemy==3.0.5
   Flask-Migrate==4.0.5
   Flask-JWT-Extended==4.5.2
   Flask-CORS==4.0.0
   Flask-WTF==1.1.1
   marshmallow==3.20.1
   redis==4.6.0
   rq==1.15.1
   PyMySQL==1.1.0
   python-dotenv==1.0.0
   requests==2.31.0

   # AIæœåŠ¡ç›¸å…³ä¾èµ– - ç»Ÿä¸€ä½¿ç”¨Langchainé›†æˆ
   langchain>=0.2.0
   langchain-community>=0.2.0
   langchain_openai>=0.1.0
   dashscope>=1.14.1

   # é˜¿é‡Œäº‘æ–‡æ¡£æ™ºèƒ½æœåŠ¡
   alibabacloud_tea_openapi
   alibabacloud_docmind_api20220711==1.4.7
   alibabacloud_credentials

   # å‘é‡æ•°æ®åº“
   weaviate-client==3.25.3
   ```

4. **åˆ›å»ºåº”ç”¨å·¥å‚** (`app/__init__.py`)
   ```python
   from flask import Flask
   from flask_sqlalchemy import SQLAlchemy
   from flask_migrate import Migrate
   from flask_jwt_extended import JWTManager
   from flask_cors import CORS
   from config import Config
   
   db = SQLAlchemy()
   migrate = Migrate()
   jwt = JWTManager()
   
   def create_app(config_class=Config):
       app = Flask(__name__)
       app.config.from_object(config_class)
       
       # åˆå§‹åŒ–æ‰©å±•
       db.init_app(app)
       migrate.init_app(app, db)
       jwt.init_app(app)
       CORS(app)
       
       # æ³¨å†Œè“å›¾
       from app.api import bp as api_bp
       app.register_blueprint(api_bp, url_prefix='/api/v1')
       
       return app
   ```

**éªŒæ”¶æ ‡å‡†**:
- Flaskåº”ç”¨èƒ½æ­£å¸¸å¯åŠ¨
- é¡¹ç›®ç»“æ„æ¸…æ™°åˆç†
- ä¾èµ–åŒ…å®‰è£…å®Œæˆ

### 1.2 é…ç½®ç®¡ç† (ç¬¬1å¤©)

**ç›®æ ‡**: è®¾ç½®é…ç½®æ–‡ä»¶å’Œç¯å¢ƒå˜é‡ç®¡ç†ã€‚

**å…·ä½“æ­¥éª¤**:

1. **åˆ›å»ºé…ç½®æ–‡ä»¶** (`config.py`)
   ```python
   import os
   from dotenv import load_dotenv
   
   basedir = os.path.abspath(os.path.dirname(__file__))
   load_dotenv(os.path.join(basedir, '.env'))
   
   class Config:
       SECRET_KEY = os.environ.get('SECRET_KEY') or 'dev-secret-key'
       
       # æ•°æ®åº“é…ç½®
       SQLALCHEMY_DATABASE_URI = os.environ.get('DATABASE_URL') or \
           'mysql+pymysql://root:password@localhost/ip_expert'
       SQLALCHEMY_TRACK_MODIFICATIONS = False
       
       # JWTé…ç½®
       JWT_SECRET_KEY = os.environ.get('JWT_SECRET_KEY') or 'jwt-secret'
       JWT_ACCESS_TOKEN_EXPIRES = 3600  # 1å°æ—¶
       
       # Redisé…ç½®
       REDIS_URL = os.environ.get('REDIS_URL') or 'redis://localhost:6379'
       
       # æ–‡ä»¶ä¸Šä¼ é…ç½®
       UPLOAD_FOLDER = os.environ.get('UPLOAD_FOLDER') or 'uploads'
       MAX_CONTENT_LENGTH = 16 * 1024 * 1024  # 16MB
   ```

2. **åˆ›å»ºç¯å¢ƒå˜é‡æ–‡ä»¶** (`.env`)
   ```bash
   SECRET_KEY=your-secret-key-here
   JWT_SECRET_KEY=your-jwt-secret-here
   DATABASE_URL=mysql+pymysql://root:password@localhost/ip_expert
   REDIS_URL=redis://localhost:6379

   # AIæœåŠ¡ç›¸å…³ - Langchainç»Ÿä¸€é›†æˆé…ç½®
   DASHSCOPE_API_KEY=your-dashscope-api-key
   OPENAI_API_KEY=your-dashscope-api-key  # ç”¨äºlangchain_openaiå…¼å®¹æ¥å£
   OPENAI_API_BASE=https://dashscope.aliyuncs.com/compatible-mode/v1

   # é˜¿é‡Œäº‘æ–‡æ¡£æ™ºèƒ½æœåŠ¡
   ALIBABA_ACCESS_KEY_ID=your-access-key-id
   ALIBABA_ACCESS_KEY_SECRET=your-access-key-secret

   # OLLAMAæœ¬åœ°é‡æ’åºæ¨¡å‹æœåŠ¡
   OLLAMA_BASE_URL=http://localhost:11434
   ```

**éªŒæ”¶æ ‡å‡†**:
- é…ç½®æ–‡ä»¶ç»“æ„åˆç†
- ç¯å¢ƒå˜é‡æ­£ç¡®åŠ è½½
- æ”¯æŒä¸åŒç¯å¢ƒé…ç½®

### 1.3 JWTè®¤è¯ç³»ç»Ÿ (ç¬¬2å¤©)

**ç›®æ ‡**: å®ç°åŸºäºJWTçš„ç”¨æˆ·è®¤è¯ç³»ç»Ÿã€‚

**å…·ä½“æ­¥éª¤**:

1. **åˆ›å»ºç”¨æˆ·æ¨¡å‹** (`app/models/user.py`)
   ```python
   from app import db
   from werkzeug.security import generate_password_hash, check_password_hash
   from datetime import datetime
   
   class User(db.Model):
       id = db.Column(db.Integer, primary_key=True)
       username = db.Column(db.String(80), unique=True, nullable=False)
       email = db.Column(db.String(120), unique=True, nullable=False)
       password_hash = db.Column(db.String(255), nullable=False)
       roles = db.Column(db.String(100), default='user')
       created_at = db.Column(db.DateTime, default=datetime.utcnow)
       is_active = db.Column(db.Boolean, default=True)
       
       def set_password(self, password):
           self.password_hash = generate_password_hash(password)
           
       def check_password(self, password):
           return check_password_hash(self.password_hash, password)
           
       def to_dict(self):
           return {
               'id': self.id,
               'username': self.username,
               'email': self.email,
               'roles': self.roles.split(',') if self.roles else []
           }
   ```

2. **å®ç°è®¤è¯API** (`app/api/auth.py`)
   ```python
   from flask import Blueprint, request, jsonify
   from flask_jwt_extended import create_access_token, jwt_required, get_jwt_identity
   from app.models.user import User
   from app import db
   
   bp = Blueprint('auth', __name__)
   
   @bp.route('/login', methods=['POST'])
   def login():
       data = request.get_json()
       username = data.get('username')
       password = data.get('password')
       
       user = User.query.filter_by(username=username).first()
       
       if user and user.check_password(password) and user.is_active:
           access_token = create_access_token(identity=user.id)
           return jsonify({
               'access_token': access_token,
               'token_type': 'Bearer',
               'user_info': user.to_dict()
           })
       
       return jsonify({'error': 'ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯'}), 401
   
   @bp.route('/me', methods=['GET'])
   @jwt_required()
   def get_current_user():
       user_id = get_jwt_identity()
       user = User.query.get(user_id)
       return jsonify(user.to_dict())
   ```

**éªŒæ”¶æ ‡å‡†**:
- ç”¨æˆ·ç™»å½•åŠŸèƒ½æ­£å¸¸
- JWT tokenç”Ÿæˆå’ŒéªŒè¯æ­£ç¡®
- å—ä¿æŠ¤çš„APIéœ€è¦è®¤è¯

---

## ğŸ¯ ä»»åŠ¡2: æ•°æ®åº“è®¾è®¡ä¸æ¨¡å‹å®šä¹‰

### 2.1 æ•°æ®åº“è®¾è®¡ (ç¬¬1å¤©)

**ç›®æ ‡**: è®¾è®¡å®Œæ•´çš„æ•°æ®åº“è¡¨ç»“æ„ã€‚

**å…·ä½“æ­¥éª¤**:

1. **æ ¸å¿ƒè¡¨è®¾è®¡**
   - users: ç”¨æˆ·è¡¨
   - cases: è¯Šæ–­æ¡ˆä¾‹è¡¨
   - nodes: èŠ‚ç‚¹è¡¨
   - edges: è¾¹è¡¨
   - knowledge_documents: çŸ¥è¯†æ–‡æ¡£è¡¨
   - parsing_jobs: è§£æä»»åŠ¡è¡¨
   - feedback: åé¦ˆè¡¨

2. **åˆ›å»ºæ•°æ®åº“** (MySQL)
   ```sql
   CREATE DATABASE ip_expert CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
   ```

3. **å¯åŠ¨MySQLæœåŠ¡** (Docker)
   ```yaml
   # docker-compose.local.yml ä¸­æ·»åŠ 
   mysql:
     image: mysql:8.0
     ports:
       - "3306:3306"
     environment:
       MYSQL_ROOT_PASSWORD: password
       MYSQL_DATABASE: ip_expert
     volumes:
       - mysql_data:/var/lib/mysql
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ•°æ®åº“è®¾è®¡æ–‡æ¡£å®Œæˆ
- MySQLæœåŠ¡æ­£å¸¸è¿è¡Œ
- æ•°æ®åº“è¿æ¥æµ‹è¯•é€šè¿‡

### 2.2 SQLAlchemyæ¨¡å‹å®šä¹‰ (ç¬¬2å¤©)

**ç›®æ ‡**: å®šä¹‰æ‰€æœ‰æ•°æ®åº“æ¨¡å‹ã€‚

**å…·ä½“æ­¥éª¤**:

1. **æ¡ˆä¾‹ç›¸å…³æ¨¡å‹** (`app/models/case.py`)
   ```python
   from app import db
   from datetime import datetime
   import uuid
   
   class Case(db.Model):
       __tablename__ = 'cases'
       
       id = db.Column(db.String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
       title = db.Column(db.String(200), nullable=False)
       status = db.Column(db.Enum('open', 'solved', 'closed'), default='open')
       user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)
       created_at = db.Column(db.DateTime, default=datetime.utcnow)
       updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
       
       # å…³ç³»
       nodes = db.relationship('Node', backref='case', lazy='dynamic', cascade='all, delete-orphan')
       edges = db.relationship('Edge', backref='case', lazy='dynamic', cascade='all, delete-orphan')
   
   class Node(db.Model):
       __tablename__ = 'nodes'
       
       id = db.Column(db.String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
       case_id = db.Column(db.String(36), db.ForeignKey('cases.id'), nullable=False)
       type = db.Column(db.Enum('USER_QUERY', 'AI_ANALYSIS', 'AI_CLARIFICATION', 'USER_RESPONSE', 'SOLUTION'))
       title = db.Column(db.String(200))
       status = db.Column(db.Enum('COMPLETED', 'AWAITING_USER_INPUT', 'PROCESSING'), default='PROCESSING')
       content = db.Column(db.JSON)
       metadata = db.Column(db.JSON)
       created_at = db.Column(db.DateTime, default=datetime.utcnow)
   
   class Edge(db.Model):
       __tablename__ = 'edges'
       
       id = db.Column(db.Integer, primary_key=True)
       case_id = db.Column(db.String(36), db.ForeignKey('cases.id'), nullable=False)
       source = db.Column(db.String(36), nullable=False)
       target = db.Column(db.String(36), nullable=False)
   ```

2. **çŸ¥è¯†æ–‡æ¡£æ¨¡å‹** (`app/models/knowledge.py`)
   ```python
   class KnowledgeDocument(db.Model):
       __tablename__ = 'knowledge_documents'
       
       id = db.Column(db.String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
       filename = db.Column(db.String(255), nullable=False)
       original_filename = db.Column(db.String(255), nullable=False)
       file_path = db.Column(db.String(500), nullable=False)
       file_size = db.Column(db.Integer)
       mime_type = db.Column(db.String(100))
       
       # å…ƒæ•°æ®
       vendor = db.Column(db.String(50))
       tags = db.Column(db.JSON)
       
       # å¤„ç†çŠ¶æ€
       status = db.Column(db.Enum('QUEUED', 'PARSING', 'INDEXED', 'FAILED'), default='QUEUED')
       progress = db.Column(db.Integer, default=0)
       error_message = db.Column(db.Text)
       
       # æ—¶é—´æˆ³
       uploaded_at = db.Column(db.DateTime, default=datetime.utcnow)
       processed_at = db.Column(db.DateTime)
       
       user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)
   
   class ParsingJob(db.Model):
       __tablename__ = 'parsing_jobs'
       
       id = db.Column(db.String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
       document_id = db.Column(db.String(36), db.ForeignKey('knowledge_documents.id'), nullable=False)
       status = db.Column(db.Enum('PENDING', 'PROCESSING', 'COMPLETED', 'FAILED'), default='PENDING')
       started_at = db.Column(db.DateTime)
       completed_at = db.Column(db.DateTime)
       error_message = db.Column(db.Text)
       result_data = db.Column(db.JSON)
       
       created_at = db.Column(db.DateTime, default=datetime.utcnow)
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ‰€æœ‰æ¨¡å‹å®šä¹‰å®Œæˆ
- å…³ç³»æ˜ å°„æ­£ç¡®
- æ•°æ®åº“è¿ç§»è„šæœ¬ç”ŸæˆæˆåŠŸ

### 2.3 æ•°æ®åº“è¿ç§» (ç¬¬3å¤©)

**ç›®æ ‡**: åˆ›å»ºå’Œæ‰§è¡Œæ•°æ®åº“è¿ç§»ã€‚

**å…·ä½“æ­¥éª¤**:

1. **åˆå§‹åŒ–è¿ç§»**
   ```bash
   flask db init
   flask db migrate -m "Initial migration"
   flask db upgrade
   ```

2. **åˆ›å»ºåˆå§‹æ•°æ®** (`scripts/init_data.py`)
   ```python
   from app import create_app, db
   from app.models.user import User
   
   def create_admin_user():
       app = create_app()
       with app.app_context():
           admin = User(
               username='admin',
               email='admin@example.com',
               roles='admin,user'
           )
           admin.set_password('admin123')
           db.session.add(admin)
           db.session.commit()
           print("Admin user created successfully")
   
   if __name__ == '__main__':
       create_admin_user()
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ•°æ®åº“è¡¨åˆ›å»ºæˆåŠŸ
- è¿ç§»è„šæœ¬æ­£å¸¸å·¥ä½œ
- åˆå§‹ç®¡ç†å‘˜ç”¨æˆ·åˆ›å»ºæˆåŠŸ

---

## ğŸ¯ ä»»åŠ¡3: æ ¸å¿ƒä¸šåŠ¡APIå®ç°

### 3.1 æ¡ˆä¾‹ç®¡ç†API (ç¬¬1-2å¤©)

**ç›®æ ‡**: å®ç°è¯Šæ–­æ¡ˆä¾‹çš„CRUDæ“ä½œã€‚

**å…·ä½“æ­¥éª¤**:

1. **æ¡ˆä¾‹åˆ—è¡¨API** (`app/api/cases.py`)
   ```python
   from flask import Blueprint, request, jsonify
   from flask_jwt_extended import jwt_required, get_jwt_identity
   from app.models.case import Case, Node, Edge
   from app import db

   bp = Blueprint('cases', __name__)

   @bp.route('/cases', methods=['GET'])
   @jwt_required()
   def get_cases():
       user_id = get_jwt_identity()

       # è·å–æŸ¥è¯¢å‚æ•°
       status = request.args.get('status')
       vendor = request.args.get('vendor')
       category = request.args.get('category')

       query = Case.query.filter_by(user_id=user_id)

       if status:
           query = query.filter_by(status=status)

       cases = query.order_by(Case.updated_at.desc()).all()

       return jsonify([{
           'caseId': case.id,
           'title': case.title,
           'status': case.status,
           'createdAt': case.created_at.isoformat() + 'Z',
           'updatedAt': case.updated_at.isoformat() + 'Z'
       } for case in cases])
   ```

2. **åˆ›å»ºæ¡ˆä¾‹API**
   ```python
   @bp.route('/cases', methods=['POST'])
   @jwt_required()
   def create_case():
       user_id = get_jwt_identity()
       data = request.get_json()

       query = data.get('query')
       attachments = data.get('attachments', [])

       if not query:
           return jsonify({'error': 'é—®é¢˜æè¿°ä¸èƒ½ä¸ºç©º'}), 400

       # åˆ›å»ºæ¡ˆä¾‹
       case = Case(
           title=query[:100] + '...' if len(query) > 100 else query,
           user_id=user_id
       )
       db.session.add(case)
       db.session.flush()  # è·å–case.id

       # åˆ›å»ºç”¨æˆ·é—®é¢˜èŠ‚ç‚¹
       user_node = Node(
           case_id=case.id,
           type='USER_QUERY',
           title='ç”¨æˆ·é—®é¢˜',
           status='COMPLETED',
           content={
               'text': query,
               'attachments': attachments
           }
       )
       db.session.add(user_node)
       db.session.flush()

       # åˆ›å»ºAIåˆ†æèŠ‚ç‚¹
       ai_node = Node(
           case_id=case.id,
           type='AI_ANALYSIS',
           title='AIåˆ†æä¸­...',
           status='PROCESSING'
       )
       db.session.add(ai_node)
       db.session.flush()

       # åˆ›å»ºè¾¹
       edge = Edge(
           case_id=case.id,
           source=user_node.id,
           target=ai_node.id
       )
       db.session.add(edge)

       db.session.commit()

       # è§¦å‘å¼‚æ­¥AIåˆ†æä»»åŠ¡
       from app.services.agent_service import analyze_user_query
       analyze_user_query.delay(case.id, ai_node.id, query)

       return jsonify({
           'caseId': case.id,
           'title': case.title,
           'nodes': [
               {
                   'id': user_node.id,
                   'type': user_node.type,
                   'title': user_node.title,
                   'status': user_node.status,
                   'content': user_node.content
               },
               {
                   'id': ai_node.id,
                   'type': ai_node.type,
                   'title': ai_node.title,
                   'status': ai_node.status
               }
           ],
           'edges': [
               {
                   'source': edge.source,
                   'target': edge.target
               }
           ]
       })
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ¡ˆä¾‹CRUDæ“ä½œæ­£å¸¸
- æ”¯æŒæŸ¥è¯¢å‚æ•°è¿‡æ»¤
- è¿”å›æ•°æ®æ ¼å¼ç¬¦åˆAPIæ–‡æ¡£

### 3.2 å¤šè½®äº¤äº’API (ç¬¬2-3å¤©)

**ç›®æ ‡**: å®ç°Agentå¤šè½®å¯¹è¯äº¤äº’ã€‚

**å…·ä½“æ­¥éª¤**:

1. **äº¤äº’å¤„ç†API**
   ```python
   @bp.route('/cases/<case_id>/interactions', methods=['POST'])
   @jwt_required()
   def handle_interaction(case_id):
       user_id = get_jwt_identity()
       data = request.get_json()

       case = Case.query.filter_by(id=case_id, user_id=user_id).first()
       if not case:
           return jsonify({'error': 'æ¡ˆä¾‹ä¸å­˜åœ¨'}), 404

       parent_node_id = data.get('parentNodeId')
       response_data = data.get('response')
       retrieval_weight = data.get('retrievalWeight', 0.7)
       filter_tags = data.get('filterTags', [])

       # åˆ›å»ºç”¨æˆ·å“åº”èŠ‚ç‚¹
       user_response_node = Node(
           case_id=case_id,
           type='USER_RESPONSE',
           title='ç”¨æˆ·è¡¥å……ä¿¡æ¯',
           status='COMPLETED',
           content=response_data
       )
       db.session.add(user_response_node)
       db.session.flush()

       # åˆ›å»ºAIå¤„ç†èŠ‚ç‚¹
       ai_processing_node = Node(
           case_id=case_id,
           type='AI_ANALYSIS',
           title='AIåˆ†æä¸­...',
           status='PROCESSING'
       )
       db.session.add(ai_processing_node)
       db.session.flush()

       # åˆ›å»ºè¾¹
       edge1 = Edge(case_id=case_id, source=parent_node_id, target=user_response_node.id)
       edge2 = Edge(case_id=case_id, source=user_response_node.id, target=ai_processing_node.id)
       db.session.add_all([edge1, edge2])

       db.session.commit()

       # è§¦å‘å¼‚æ­¥å¤„ç†
       from app.services.agent_service import process_user_response
       process_user_response.delay(
           case_id,
           ai_processing_node.id,
           response_data,
           retrieval_weight,
           filter_tags
       )

       return jsonify({
           'newNodes': [
               {
                   'id': user_response_node.id,
                   'type': user_response_node.type,
                   'title': user_response_node.title,
                   'status': user_response_node.status,
                   'content': user_response_node.content
               },
               {
                   'id': ai_processing_node.id,
                   'type': ai_processing_node.type,
                   'title': ai_processing_node.title,
                   'status': ai_processing_node.status
               }
           ],
           'newEdges': [
               {'source': edge1.source, 'target': edge1.target},
               {'source': edge2.source, 'target': edge2.target}
           ],
           'processingNodeId': ai_processing_node.id
       })
   ```

2. **èŠ‚ç‚¹è¯¦æƒ…API**
   ```python
   @bp.route('/cases/<case_id>/nodes/<node_id>', methods=['GET'])
   @jwt_required()
   def get_node_detail(case_id, node_id):
       user_id = get_jwt_identity()

       case = Case.query.filter_by(id=case_id, user_id=user_id).first()
       if not case:
           return jsonify({'error': 'æ¡ˆä¾‹ä¸å­˜åœ¨'}), 404

       node = Node.query.filter_by(id=node_id, case_id=case_id).first()
       if not node:
           return jsonify({'error': 'èŠ‚ç‚¹ä¸å­˜åœ¨'}), 404

       return jsonify({
           'id': node.id,
           'type': node.type,
           'title': node.title,
           'status': node.status,
           'content': node.content,
           'metadata': node.metadata
       })
   ```

**éªŒæ”¶æ ‡å‡†**:
- å¤šè½®äº¤äº’æµç¨‹æ­£å¸¸
- èŠ‚ç‚¹çŠ¶æ€æ›´æ–°åŠæ—¶
- å¼‚æ­¥ä»»åŠ¡æ­£ç¡®è§¦å‘

### 3.3 åé¦ˆå¤„ç†API (ç¬¬3å¤©)

**ç›®æ ‡**: å®ç°ç”¨æˆ·åé¦ˆæ”¶é›†å’Œå¤„ç†ã€‚

**å…·ä½“æ­¥éª¤**:

1. **åé¦ˆæ¨¡å‹** (`app/models/feedback.py`)
   ```python
   class Feedback(db.Model):
       __tablename__ = 'feedback'

       id = db.Column(db.String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
       case_id = db.Column(db.String(36), db.ForeignKey('cases.id'), nullable=False)
       user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)

       outcome = db.Column(db.Enum('solved', 'unsolved'), nullable=False)
       comment = db.Column(db.Text)
       corrected_solution = db.Column(db.JSON)

       # å®¡æ ¸çŠ¶æ€
       review_status = db.Column(db.Enum('pending', 'approved', 'rejected'), default='pending')
       reviewed_by = db.Column(db.Integer, db.ForeignKey('user.id'))
       reviewed_at = db.Column(db.DateTime)

       created_at = db.Column(db.DateTime, default=datetime.utcnow)
   ```

2. **åé¦ˆAPI**
   ```python
   @bp.route('/cases/<case_id>/feedback', methods=['POST'])
   @jwt_required()
   def submit_feedback(case_id):
       user_id = get_jwt_identity()
       data = request.get_json()

       case = Case.query.filter_by(id=case_id, user_id=user_id).first()
       if not case:
           return jsonify({'error': 'æ¡ˆä¾‹ä¸å­˜åœ¨'}), 404

       feedback = Feedback(
           case_id=case_id,
           user_id=user_id,
           outcome=data.get('outcome'),
           comment=data.get('comment'),
           corrected_solution=data.get('corrected_solution')
       )

       db.session.add(feedback)

       # æ›´æ–°æ¡ˆä¾‹çŠ¶æ€
       if data.get('outcome') == 'solved':
           case.status = 'solved'

       db.session.commit()

       return jsonify({
           'status': 'success',
           'message': 'åé¦ˆå·²æ”¶åˆ°'
       })
   ```

**éªŒæ”¶æ ‡å‡†**:
- åé¦ˆæ•°æ®æ­£ç¡®ä¿å­˜
- æ¡ˆä¾‹çŠ¶æ€æ­£ç¡®æ›´æ–°
- æ”¯æŒè§£å†³æ–¹æ¡ˆä¿®æ­£

---

## ğŸ¯ ä»»åŠ¡4: çŸ¥è¯†æ–‡æ¡£ç®¡ç†API

### 4.1 æ–‡ä»¶ä¸Šä¼ API (ç¬¬1å¤©)

**ç›®æ ‡**: å®ç°çŸ¥è¯†æ–‡æ¡£çš„ä¸Šä¼ åŠŸèƒ½ã€‚

**å…·ä½“æ­¥éª¤**:

1. **æ–‡ä»¶ä¸Šä¼ å¤„ç†** (`app/api/knowledge.py`)
   ```python
   import os
   from werkzeug.utils import secure_filename
   from flask import Blueprint, request, jsonify, current_app
   from flask_jwt_extended import jwt_required, get_jwt_identity

   bp = Blueprint('knowledge', __name__)

   ALLOWED_EXTENSIONS = {'pdf', 'doc', 'docx', 'txt', 'md', 'png', 'jpg', 'jpeg'}

   def allowed_file(filename):
       return '.' in filename and \
              filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

   @bp.route('/knowledge/documents', methods=['POST'])
   @jwt_required()
   def upload_document():
       user_id = get_jwt_identity()

       if 'file' not in request.files:
           return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400

       file = request.files['file']
       if file.filename == '':
           return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400

       if not allowed_file(file.filename):
           return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹'}), 400

       # è·å–è¡¨å•æ•°æ®
       tags = request.form.getlist('tags')
       vendor = request.form.get('vendor')

       # ä¿å­˜æ–‡ä»¶
       filename = secure_filename(file.filename)
       file_id = str(uuid.uuid4())
       file_extension = filename.rsplit('.', 1)[1].lower()
       new_filename = f"{file_id}.{file_extension}"

       upload_folder = current_app.config['UPLOAD_FOLDER']
       os.makedirs(upload_folder, exist_ok=True)
       file_path = os.path.join(upload_folder, new_filename)
       file.save(file_path)

       # ä¿å­˜æ–‡æ¡£è®°å½•
       document = KnowledgeDocument(
           id=file_id,
           filename=new_filename,
           original_filename=filename,
           file_path=file_path,
           file_size=os.path.getsize(file_path),
           mime_type=file.mimetype,
           vendor=vendor,
           tags=tags,
           user_id=user_id
       )

       db.session.add(document)
       db.session.flush()

       # åˆ›å»ºè§£æä»»åŠ¡
       parsing_job = ParsingJob(
           document_id=document.id
       )
       db.session.add(parsing_job)
       db.session.commit()

       # è§¦å‘å¼‚æ­¥è§£æä»»åŠ¡
       from app.services.document_service import parse_document
       parse_document.delay(parsing_job.id)

       return jsonify({
           'docId': document.id,
           'status': 'QUEUED',
           'message': 'æ–‡æ¡£å·²åŠ å…¥å¤„ç†é˜Ÿåˆ—'
       })
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½æ­£å¸¸
- æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
- æ–‡ä»¶å®‰å…¨æ€§æ£€æŸ¥å®Œå–„

### 4.2 æ–‡æ¡£åˆ—è¡¨å’Œè¯¦æƒ…API (ç¬¬2å¤©)

**ç›®æ ‡**: å®ç°æ–‡æ¡£çš„æŸ¥è¯¢å’Œç®¡ç†åŠŸèƒ½ã€‚

**å…·ä½“æ­¥éª¤**:

1. **æ–‡æ¡£åˆ—è¡¨API**
   ```python
   @bp.route('/knowledge/documents', methods=['GET'])
   @jwt_required()
   def get_documents():
       user_id = get_jwt_identity()

       # è·å–æŸ¥è¯¢å‚æ•°
       status = request.args.get('status')
       vendor = request.args.get('vendor')
       page = int(request.args.get('page', 1))
       page_size = int(request.args.get('pageSize', 10))

       query = KnowledgeDocument.query.filter_by(user_id=user_id)

       if status:
           query = query.filter_by(status=status)
       if vendor:
           query = query.filter_by(vendor=vendor)

       # åˆ†é¡µ
       pagination = query.paginate(
           page=page,
           per_page=page_size,
           error_out=False
       )

       documents = pagination.items

       return jsonify({
           'items': [{
               'docId': doc.id,
               'fileName': doc.original_filename,
               'vendor': doc.vendor,
               'tags': doc.tags,
               'status': doc.status,
               'uploadedAt': doc.uploaded_at.isoformat() + 'Z'
           } for doc in documents],
           'total': pagination.total,
           'page': page,
           'pageSize': page_size
       })
   ```

2. **æ–‡æ¡£è¯¦æƒ…API**
   ```python
   @bp.route('/knowledge/documents/<doc_id>', methods=['GET'])
   @jwt_required()
   def get_document_detail(doc_id):
       user_id = get_jwt_identity()

       document = KnowledgeDocument.query.filter_by(
           id=doc_id,
           user_id=user_id
       ).first()

       if not document:
           return jsonify({'error': 'æ–‡æ¡£ä¸å­˜åœ¨'}), 404

       # è·å–è§£æä»»åŠ¡ä¿¡æ¯
       parsing_job = ParsingJob.query.filter_by(
           document_id=doc_id
       ).order_by(ParsingJob.created_at.desc()).first()

       return jsonify({
           'docId': document.id,
           'fileName': document.original_filename,
           'vendor': document.vendor,
           'tags': document.tags,
           'status': document.status,
           'progress': document.progress,
           'message': parsing_job.error_message if parsing_job else None,
           'createdAt': document.uploaded_at.isoformat() + 'Z',
           'updatedAt': document.processed_at.isoformat() + 'Z' if document.processed_at else None
       })
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ–‡æ¡£åˆ—è¡¨æŸ¥è¯¢æ­£å¸¸
- æ”¯æŒåˆ†é¡µå’Œè¿‡æ»¤
- æ–‡æ¡£è¯¦æƒ…ä¿¡æ¯å®Œæ•´

### 4.3 æ–‡æ¡£åˆ é™¤å’Œé‡è§£æAPI (ç¬¬3å¤©)

**ç›®æ ‡**: å®ç°æ–‡æ¡£çš„åˆ é™¤å’Œé‡æ–°è§£æåŠŸèƒ½ã€‚

**å…·ä½“æ­¥éª¤**:

1. **åˆ é™¤æ–‡æ¡£API**
   ```python
   @bp.route('/knowledge/documents/<doc_id>', methods=['DELETE'])
   @jwt_required()
   def delete_document(doc_id):
       user_id = get_jwt_identity()

       document = KnowledgeDocument.query.filter_by(
           id=doc_id,
           user_id=user_id
       ).first()

       if not document:
           return jsonify({'error': 'æ–‡æ¡£ä¸å­˜åœ¨'}), 404

       # åˆ é™¤ç‰©ç†æ–‡ä»¶
       if os.path.exists(document.file_path):
           os.remove(document.file_path)

       # åˆ é™¤å‘é‡æ•°æ®åº“ä¸­çš„æ•°æ®
       from app.services.vector_service import delete_document_vectors
       delete_document_vectors.delay(doc_id)

       # åˆ é™¤æ•°æ®åº“è®°å½•
       db.session.delete(document)
       db.session.commit()

       return '', 204
   ```

2. **é‡æ–°è§£æAPI**
   ```python
   @bp.route('/knowledge/documents/<doc_id>/reparse', methods=['PUT'])
   @jwt_required()
   def reparse_document(doc_id):
       user_id = get_jwt_identity()

       document = KnowledgeDocument.query.filter_by(
           id=doc_id,
           user_id=user_id
       ).first()

       if not document:
           return jsonify({'error': 'æ–‡æ¡£ä¸å­˜åœ¨'}), 404

       # é‡ç½®çŠ¶æ€
       document.status = 'QUEUED'
       document.progress = 0
       document.error_message = None

       # åˆ›å»ºæ–°çš„è§£æä»»åŠ¡
       parsing_job = ParsingJob(
           document_id=document.id
       )
       db.session.add(parsing_job)
       db.session.commit()

       # è§¦å‘å¼‚æ­¥è§£æ
       from app.services.document_service import parse_document
       parse_document.delay(parsing_job.id)

       return jsonify({
           'docId': document.id,
           'status': 'PARSING',
           'message': 'å·²è§¦å‘é‡æ–°è§£æ'
       })
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ–‡æ¡£åˆ é™¤åŠŸèƒ½æ­£å¸¸
- é‡æ–°è§£æåŠŸèƒ½æ­£å¸¸
- ç›¸å…³æ•°æ®æ¸…ç†å®Œæ•´

---

## ğŸ¯ ä»»åŠ¡5: å¼‚æ­¥ä»»åŠ¡å¤„ç†ç³»ç»Ÿ

### 5.1 RQä»»åŠ¡é˜Ÿåˆ—é…ç½® (ç¬¬1å¤©)

**ç›®æ ‡**: é…ç½®Rediså’ŒRQä»»åŠ¡é˜Ÿåˆ—ç³»ç»Ÿã€‚

**å…·ä½“æ­¥éª¤**:

1. **Redisé…ç½®** (docker-compose.local.yml)
   ```yaml
   redis:
     image: redis:7-alpine
     ports:
       - "6379:6379"
     volumes:
       - redis_data:/data
   ```

2. **RQé…ç½®** (`app/services/__init__.py`)
   ```python
   import redis
   from rq import Queue
   from flask import current_app

   def get_redis_connection():
       return redis.from_url(current_app.config['REDIS_URL'])

   def get_task_queue():
       return Queue('default', connection=get_redis_connection())
   ```

3. **Workerå¯åŠ¨è„šæœ¬** (`worker.py`)
   ```python
   import os
   from rq import Worker
   from app import create_app
   from app.services import get_redis_connection

   if __name__ == '__main__':
       app = create_app()
       with app.app_context():
           redis_conn = get_redis_connection()
           worker = Worker(['default'], connection=redis_conn)
           worker.work()
   ```

**éªŒæ”¶æ ‡å‡†**:
- RedisæœåŠ¡æ­£å¸¸è¿è¡Œ
- RQé˜Ÿåˆ—é…ç½®æ­£ç¡®
- Workerè¿›ç¨‹èƒ½æ­£å¸¸å¯åŠ¨

### 5.2 æ–‡æ¡£è§£ææœåŠ¡ (ç¬¬2å¤©)

**ç›®æ ‡**: å®ç°å¼‚æ­¥æ–‡æ¡£è§£ææœåŠ¡ã€‚

**å…·ä½“æ­¥éª¤**:

1. **æ–‡æ¡£è§£æä»»åŠ¡** (`app/services/document_service.py`)
   ```python
   from rq import get_current_job
   from app import create_app, db
   from app.models.knowledge import KnowledgeDocument, ParsingJob
   from app.services.idp_service import IDPService
   from app.services.vector_service import VectorService

   def parse_document(job_id):
       app = create_app()
       with app.app_context():
           job = ParsingJob.query.get(job_id)
           if not job:
               return

           document = KnowledgeDocument.query.get(job.document_id)
           if not document:
               return

           try:
               # æ›´æ–°çŠ¶æ€
               job.status = 'PROCESSING'
               document.status = 'PARSING'
               db.session.commit()

               # è°ƒç”¨IDPæœåŠ¡è§£ææ–‡æ¡£
               idp_service = IDPService()
               parsed_result = idp_service.parse_document(document.file_path)

               # è¯­ä¹‰åˆ‡åˆ†
               from app.services.semantic_splitter import SemanticSplitter
               splitter = SemanticSplitter()
               chunks = splitter.split_document(parsed_result, document)

               # å‘é‡åŒ–å¹¶å­˜å‚¨
               vector_service = VectorService()
               vector_service.index_chunks(chunks, document.id)

               # æ›´æ–°çŠ¶æ€
               job.status = 'COMPLETED'
               job.result_data = {'chunks_count': len(chunks)}
               document.status = 'INDEXED'
               document.progress = 100
               document.processed_at = datetime.utcnow()

               db.session.commit()

           except Exception as e:
               # é”™è¯¯å¤„ç†
               job.status = 'FAILED'
               job.error_message = str(e)
               document.status = 'FAILED'
               document.error_message = str(e)

               db.session.commit()
               raise
   ```

2. **IDPæœåŠ¡å°è£…** (`app/services/idp_service.py`)
   ```python
   from alibabacloud_docmind_api20220711.client import Client as docmind_api20220711Client
   from alibabacloud_tea_openapi import models as open_api_models
   from alibabacloud_docmind_api20220711 import models as docmind_api20220711_models
   from alibabacloud_tea_util import models as util_models
   from alibabacloud_credentials.client import Client as CredClient
   import os
   import time

   class IDPService:
       def __init__(self):
           # ä½¿ç”¨é»˜è®¤å‡­è¯åˆå§‹åŒ–Credentials Client
           cred = CredClient()
           config = open_api_models.Config(
               access_key_id=cred.get_credential().access_key_id,
               access_key_secret=cred.get_credential().access_key_secret
           )
           config.endpoint = 'docmind-api.cn-hangzhou.aliyuncs.com'
           self.client = docmind_api20220711Client(config)

       def parse_document(self, file_path):
           """è°ƒç”¨é˜¿é‡Œäº‘æ–‡æ¡£æ™ºèƒ½APIè§£ææ–‡æ¡£"""
           try:
               # æ­¥éª¤1: æäº¤æ–‡æ¡£è§£æä»»åŠ¡
               request = docmind_api20220711_models.SubmitDocStructureJobAdvanceRequest(
                   file_url_object=open(file_path, "rb"),
                   file_name=os.path.basename(file_path),
                   structure_type='default',  # è¿”å›å®Œæ•´ç»“æ„åŒ–ä¿¡æ¯
                   formula_enhancement=True   # å¼€å¯å…¬å¼è¯†åˆ«å¢å¼º
               )
               runtime = util_models.RuntimeOptions()

               response = self.client.submit_doc_structure_job_advance(request, runtime)
               job_id = response.body.data.id

               # æ­¥éª¤2: è½®è¯¢è·å–ç»“æœ
               while True:
                   query_request = docmind_api20220711_models.GetDocStructureResultRequest(
                       id=job_id,
                       reveal_markdown=True  # è¾“å‡ºMarkdownæ ¼å¼
                   )

                   query_response = self.client.get_doc_structure_result(query_request)

                   if query_response.body.completed:
                       if query_response.body.status == 'Success':
                           return query_response.body.data
                       else:
                           raise Exception(f"æ–‡æ¡£è§£æå¤±è´¥: {query_response.body.message}")

                   time.sleep(10)  # å»ºè®®æ¯10ç§’è½®è¯¢ä¸€æ¬¡

           except Exception as e:
               raise Exception(f"IDPæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}")

       def parse_document_from_url(self, file_url, file_name):
           """é€šè¿‡URLè§£ææ–‡æ¡£"""
           try:
               request = docmind_api20220711_models.SubmitDocStructureJobRequest(
                   file_url=file_url,
                   file_name=file_name,
                   structure_type='default',
                   formula_enhancement=True
               )

               response = self.client.submit_doc_structure_job(request)
               job_id = response.body.data.id

               # è½®è¯¢è·å–ç»“æœ (åŒä¸Šé€»è¾‘)
               while True:
                   query_request = docmind_api20220711_models.GetDocStructureResultRequest(
                       id=job_id,
                       reveal_markdown=True
                   )

                   query_response = self.client.get_doc_structure_result(query_request)

                   if query_response.body.completed:
                       if query_response.body.status == 'Success':
                           return query_response.body.data
                       else:
                           raise Exception(f"æ–‡æ¡£è§£æå¤±è´¥: {query_response.body.message}")

                   time.sleep(10)

           except Exception as e:
               raise Exception(f"IDPæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}")
   ```

**éªŒæ”¶æ ‡å‡†**:
- å¼‚æ­¥è§£æä»»åŠ¡æ­£å¸¸æ‰§è¡Œ
- IDPæœåŠ¡é›†æˆæˆåŠŸ
- é”™è¯¯å¤„ç†æœºåˆ¶å®Œå–„

### 5.3 Agentå¤„ç†æœåŠ¡ (ç¬¬3å¤©)

**ç›®æ ‡**: å®ç°Agentå¤šè½®å¯¹è¯çš„å¼‚æ­¥å¤„ç†ã€‚

**å…·ä½“æ­¥éª¤**:

1. **AgentæœåŠ¡** (`app/services/agent_service.py`)
   ```python
   from app.services.retrieval_service import RetrievalService
   from app.services.llm_service import LLMService
   from langchain_openai import ChatOpenAI
   from langchain.schema import HumanMessage, SystemMessage
   import os

   def analyze_user_query(case_id, node_id, query):
       app = create_app()
       with app.app_context():
           try:
               # è·å–èŠ‚ç‚¹
               node = Node.query.get(node_id)
               if not node:
                   return

               # é—®é¢˜åˆ†æ
               llm_service = LLMService()
               analysis_result = llm_service.analyze_query(query)

               # æ£€ç´¢ç›¸å…³çŸ¥è¯†
               retrieval_service = RetrievalService()
               context = retrieval_service.search(query)

               # åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´å¤šä¿¡æ¯
               if analysis_result.get('need_more_info'):
                   # ç”Ÿæˆè¿½é—®
                   clarification = llm_service.generate_clarification(
                       query, analysis_result, context
                   )

                   node.type = 'AI_CLARIFICATION'
                   node.title = 'éœ€è¦æ›´å¤šä¿¡æ¯'
                   node.status = 'AWAITING_USER_INPUT'
                   node.content = {
                       'analysis': analysis_result.get('analysis'),
                       'suggestedQuestions': clarification.get('questions')
                   }
               else:
                   # ç”Ÿæˆè§£å†³æ–¹æ¡ˆ
                   solution = llm_service.generate_solution(
                       query, context, analysis_result
                   )

                   node.type = 'SOLUTION'
                   node.title = 'è§£å†³æ–¹æ¡ˆ'
                   node.status = 'COMPLETED'
                   node.content = {
                       'answer': solution.get('answer'),
                       'sources': solution.get('sources'),
                       'commands': solution.get('commands')
                   }

               db.session.commit()

           except Exception as e:
               # é”™è¯¯å¤„ç†
               node.status = 'COMPLETED'
               node.content = {
                   'error': 'å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œè¯·é‡è¯•'
               }
               db.session.commit()
               raise

   def process_user_response(case_id, node_id, response_data, retrieval_weight, filter_tags):
       # å¤„ç†ç”¨æˆ·è¡¥å……ä¿¡æ¯çš„é€»è¾‘
       pass
   ```

**éªŒæ”¶æ ‡å‡†**:
- Agentå¼‚æ­¥å¤„ç†æ­£å¸¸
- å¤šè½®å¯¹è¯é€»è¾‘æ­£ç¡®
- èŠ‚ç‚¹çŠ¶æ€æ›´æ–°åŠæ—¶

### 5.4 ä»»åŠ¡ç›‘æ§å’Œé‡è¯• (ç¬¬4å¤©)

**ç›®æ ‡**: å®ç°ä»»åŠ¡ç›‘æ§å’Œå¤±è´¥é‡è¯•æœºåˆ¶ã€‚

**å…·ä½“æ­¥éª¤**:

1. **ä»»åŠ¡ç›‘æ§** (`app/services/task_monitor.py`)
   ```python
   from rq import get_current_job
   import logging

   def monitor_task_progress(func):
       def wrapper(*args, **kwargs):
           job = get_current_job()
           if job:
               job.meta['status'] = 'running'
               job.save_meta()

           try:
               result = func(*args, **kwargs)
               if job:
                   job.meta['status'] = 'completed'
                   job.save_meta()
               return result
           except Exception as e:
               if job:
                   job.meta['status'] = 'failed'
                   job.meta['error'] = str(e)
                   job.save_meta()
               logging.error(f"Task failed: {e}")
               raise
       return wrapper
   ```

2. **é‡è¯•æœºåˆ¶**
   ```python
   from rq import Retry

   # åœ¨ä»»åŠ¡è£…é¥°å™¨ä¸­æ·»åŠ é‡è¯•
   @monitor_task_progress
   def parse_document(job_id):
       # ä»»åŠ¡é€»è¾‘
       pass

   # æäº¤ä»»åŠ¡æ—¶æŒ‡å®šé‡è¯•ç­–ç•¥
   queue.enqueue(
       parse_document,
       job_id,
       retry=Retry(max=3, interval=[10, 30, 60])
   )
   ```

**éªŒæ”¶æ ‡å‡†**:
- ä»»åŠ¡ç›‘æ§åŠŸèƒ½æ­£å¸¸
- å¤±è´¥é‡è¯•æœºåˆ¶æœ‰æ•ˆ
- æ—¥å¿—è®°å½•å®Œæ•´

---

## ğŸ¯ ä»»åŠ¡6: æ•°æ®çœ‹æ¿APIå®ç°

### 6.1 ç»Ÿè®¡æ•°æ®èšåˆ (ç¬¬1å¤©)

**ç›®æ ‡**: å®ç°æ•°æ®çœ‹æ¿æ‰€éœ€çš„ç»Ÿè®¡APIã€‚

**å…·ä½“æ­¥éª¤**:

1. **ç»Ÿè®¡API** (`app/api/statistics.py`)
   ```python
   from flask import Blueprint, request, jsonify
   from flask_jwt_extended import jwt_required
   from sqlalchemy import func, and_
   from datetime import datetime, timedelta

   bp = Blueprint('statistics', __name__)

   @bp.route('/statistics', methods=['GET'])
   @jwt_required()
   def get_statistics():
       time_range = request.args.get('timeRange', '30d')

       # è®¡ç®—æ—¶é—´èŒƒå›´
       if time_range == '7d':
           start_date = datetime.utcnow() - timedelta(days=7)
       elif time_range == '90d':
           start_date = datetime.utcnow() - timedelta(days=90)
       else:  # 30d
           start_date = datetime.utcnow() - timedelta(days=30)

       # æ•…éšœåˆ†ç±»ç»Ÿè®¡
       fault_categories = db.session.query(
           Node.metadata['category'].astext.label('category'),
           func.count(Node.id).label('count')
       ).filter(
           and_(
               Node.type == 'USER_QUERY',
               Node.created_at >= start_date
           )
       ).group_by(Node.metadata['category'].astext).all()

       # è§£å†³ç‡è¶‹åŠ¿
       resolution_trend = []
       for i in range(7):
           date = datetime.utcnow() - timedelta(days=i)
           date_str = date.strftime('%Y-%m-%d')

           total_cases = Case.query.filter(
               func.date(Case.created_at) == date.date()
           ).count()

           solved_cases = Case.query.filter(
               and_(
                   func.date(Case.created_at) == date.date(),
                   Case.status == 'solved'
               )
           ).count()

           rate = solved_cases / total_cases if total_cases > 0 else 0
           resolution_trend.append({
               'date': date_str,
               'rate': round(rate, 2)
           })

       # çŸ¥è¯†è¦†ç›–åº¦
       knowledge_coverage = db.session.query(
           KnowledgeDocument.vendor,
           func.count(KnowledgeDocument.id).label('doc_count')
       ).filter(
           KnowledgeDocument.status == 'INDEXED'
       ).group_by(KnowledgeDocument.vendor).all()

       return jsonify({
           'faultCategories': [
               {'name': cat.category or 'Other', 'value': cat.count}
               for cat in fault_categories
           ],
           'resolutionTrend': resolution_trend,
           'knowledgeCoverage': [
               {
                   'topic': 'General',
                   'vendor': cov.vendor or 'Unknown',
                   'coverage': min(cov.doc_count * 10, 100)  # ç®€åŒ–è®¡ç®—
               }
               for cov in knowledge_coverage
           ]
       })
   ```

**éªŒæ”¶æ ‡å‡†**:
- ç»Ÿè®¡æ•°æ®è®¡ç®—æ­£ç¡®
- æ”¯æŒä¸åŒæ—¶é—´èŒƒå›´
- è¿”å›æ ¼å¼ç¬¦åˆå‰ç«¯éœ€æ±‚

### 6.2 æ€§èƒ½ä¼˜åŒ– (ç¬¬2å¤©)

**ç›®æ ‡**: ä¼˜åŒ–ç»Ÿè®¡æŸ¥è¯¢æ€§èƒ½ã€‚

**å…·ä½“æ­¥éª¤**:

1. **æ·»åŠ æ•°æ®åº“ç´¢å¼•**
   ```python
   # åœ¨æ¨¡å‹ä¸­æ·»åŠ ç´¢å¼•
   class Node(db.Model):
       # ... å…¶ä»–å­—æ®µ
       created_at = db.Column(db.DateTime, default=datetime.utcnow, index=True)
       type = db.Column(db.Enum(...), index=True)

   class Case(db.Model):
       # ... å…¶ä»–å­—æ®µ
       created_at = db.Column(db.DateTime, default=datetime.utcnow, index=True)
       status = db.Column(db.Enum(...), index=True)
   ```

2. **ç¼“å­˜æœºåˆ¶**
   ```python
   import redis
   import json
   from functools import wraps

   def cache_result(expire_time=300):
       def decorator(func):
           @wraps(func)
           def wrapper(*args, **kwargs):
               # ç”Ÿæˆç¼“å­˜key
               cache_key = f"stats:{func.__name__}:{hash(str(args) + str(kwargs))}"

               # å°è¯•ä»ç¼“å­˜è·å–
               redis_client = get_redis_connection()
               cached = redis_client.get(cache_key)
               if cached:
                   return json.loads(cached)

               # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
               result = func(*args, **kwargs)
               redis_client.setex(cache_key, expire_time, json.dumps(result))
               return result
           return wrapper
       return decorator

   @cache_result(expire_time=600)  # ç¼“å­˜10åˆ†é’Ÿ
   def get_statistics():
       # ç»Ÿè®¡é€»è¾‘
       pass
   ```

**éªŒæ”¶æ ‡å‡†**:
- æŸ¥è¯¢æ€§èƒ½æ˜¾è‘—æå‡
- ç¼“å­˜æœºåˆ¶å·¥ä½œæ­£å¸¸
- æ•°æ®ä¸€è‡´æ€§ä¿è¯

---

## ğŸ¯ ä»»åŠ¡7: æ•°æ®æ”¶é›†ä¸IDPé›†æˆ

### 7.1 æ•°æ®æ”¶é›†å‡†å¤‡ (ç¬¬1å¤©)

**ç›®æ ‡**: æ”¶é›†ç½‘ç»œè¿ç»´ç›¸å…³çš„çŸ¥è¯†æ–‡æ¡£ï¼Œä¸ºçŸ¥è¯†åº“æ„å»ºåšå‡†å¤‡ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿä¸»å¯¼ï¼Œåç«¯å›¢é˜ŸååŠ©

**å…·ä½“æ­¥éª¤**:

1. **åˆ›å»ºæ•°æ®ç›®å½•ç»“æ„**
   ```bash
   mkdir -p assets/datasets/raw/{rfc,manuals,cases,others}
   mkdir -p assets/datasets/processed
   mkdir -p assets/datasets/embeddings
   ```

2. **æ”¶é›†RFCæ–‡æ¡£** (é‡ç‚¹æ”¶é›†ä»¥ä¸‹RFC)
   - RFC 2328 (OSPFv2)
   - RFC 4271 (BGP-4)
   - RFC 3031 (MPLSæ¶æ„)
   - RFC 4364 (BGP/MPLS IP VPN)
   - RFC 2547 (BGP/MPLS VPN)

3. **æ”¶é›†è®¾å¤‡æ‰‹å†Œ**
   - åä¸º: VRPç³»ç»Ÿé…ç½®æŒ‡å—ã€æ•…éšœå¤„ç†æ‰‹å†Œ
   - æ€ç§‘: IOSé…ç½®æŒ‡å—ã€æ•…éšœæ’é™¤æ‰‹å†Œ
   - åä¸‰: Comwareç³»ç»Ÿæ‰‹å†Œ

4. **æ”¶é›†æ¡ˆä¾‹æ–‡æ¡£**
   - ç½‘ç»œæ•…éšœæ¡ˆä¾‹åˆ†æ
   - è¿ç»´æœ€ä½³å®è·µæ–‡æ¡£
   - æŠ€æœ¯åšå®¢å’Œè®ºå›ç²¾åå¸–

**éªŒæ”¶æ ‡å‡†**:
- æ”¶é›†åˆ°è‡³å°‘50ä¸ªé«˜è´¨é‡æ–‡æ¡£
- æ–‡æ¡£æŒ‰å‚å•†å’ŒæŠ€æœ¯åˆ†ç±»å­˜æ”¾
- å»ºç«‹æ–‡æ¡£æ¸…å•Excelè¡¨æ ¼ï¼Œè®°å½•æ–‡ä»¶åã€æ¥æºã€åˆ†ç±»ã€å‚å•†æ ‡ç­¾

### 7.2 é˜¿é‡Œäº‘æ–‡æ¡£æ™ºèƒ½æœåŠ¡é›†æˆ (ç¬¬2å¤©)

**ç›®æ ‡**: é›†æˆé˜¿é‡Œäº‘æ–‡æ¡£æ™ºèƒ½æœåŠ¡åˆ°åç«¯å¼‚æ­¥ä»»åŠ¡ç³»ç»Ÿä¸­ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿä¸»å¯¼ï¼Œåç«¯å›¢é˜Ÿé…åˆ

**å…·ä½“æ­¥éª¤**:

1. **å®Œå–„IDPæœåŠ¡å°è£…** (`app/services/idp_service.py`)
   ```python
   from alibabacloud_docmind_api20220711.client import Client as docmind_api20220711Client
   from alibabacloud_tea_openapi import models as open_api_models
   from alibabacloud_docmind_api20220711 import models as docmind_api20220711_models
   from alibabacloud_tea_util import models as util_models
   from alibabacloud_credentials.client import Client as CredClient
   import os
   import time

   class IDPService:
       def __init__(self):
           # ä½¿ç”¨é»˜è®¤å‡­è¯åˆå§‹åŒ–Credentials Client
           cred = CredClient()
           config = open_api_models.Config(
               access_key_id=cred.get_credential().access_key_id,
               access_key_secret=cred.get_credential().access_key_secret
           )
           config.endpoint = 'docmind-api.cn-hangzhou.aliyuncs.com'
           self.client = docmind_api20220711Client(config)

       def parse_document(self, file_path):
           """è°ƒç”¨é˜¿é‡Œäº‘æ–‡æ¡£æ™ºèƒ½APIè§£ææ–‡æ¡£"""
           try:
               # æ­¥éª¤1: æäº¤æ–‡æ¡£è§£æä»»åŠ¡
               request = docmind_api20220711_models.SubmitDocStructureJobAdvanceRequest(
                   file_url_object=open(file_path, "rb"),
                   file_name=os.path.basename(file_path),
                   structure_type='default',  # è¿”å›å®Œæ•´ç»“æ„åŒ–ä¿¡æ¯
                   formula_enhancement=True   # å¼€å¯å…¬å¼è¯†åˆ«å¢å¼º
               )
               runtime = util_models.RuntimeOptions()

               response = self.client.submit_doc_structure_job_advance(request, runtime)
               job_id = response.body.data.id

               # æ­¥éª¤2: è½®è¯¢è·å–ç»“æœ
               while True:
                   query_request = docmind_api20220711_models.GetDocStructureResultRequest(
                       id=job_id,
                       reveal_markdown=True  # è¾“å‡ºMarkdownæ ¼å¼
                   )

                   query_response = self.client.get_doc_structure_result(query_request)

                   if query_response.body.completed:
                       if query_response.body.status == 'Success':
                           return query_response.body.data
                       else:
                           raise Exception(f"æ–‡æ¡£è§£æå¤±è´¥: {query_response.body.message}")

                   time.sleep(10)  # å»ºè®®æ¯10ç§’è½®è¯¢ä¸€æ¬¡

           except Exception as e:
               raise Exception(f"IDPæœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}")
   ```

2. **å®ç°è¯­ä¹‰åˆ‡åˆ†å™¨** (`app/services/semantic_splitter.py`)
   ```python
   class SemanticSplitter:
       def __init__(self, max_chunk_size=1000, overlap=100):
           self.max_chunk_size = max_chunk_size
           self.overlap = overlap

       def split_document(self, idp_result, document):
           """åŸºäºIDPç»“æœè¿›è¡Œè¯­ä¹‰åˆ‡åˆ†"""
           chunks = []

           # è§£ææ–°çš„IDPè¿”å›ç»“æ„
           layouts = idp_result.get('layouts', [])
           doc_tree = idp_result.get('logics', {}).get('docTree', [])

           # æŒ‰ç…§æ–‡æ¡£å±‚çº§æ ‘è¿›è¡Œè¯­ä¹‰åˆ‡åˆ†
           for layout in layouts:
               chunk = {
                   'content': layout.get('text', ''),
                   'title': self._extract_title(layout),
                   'type': layout.get('type', 'text'),
                   'subtype': layout.get('subType', ''),
                   'page_number': layout.get('pageNum', [0])[0] if layout.get('pageNum') else 0,
                   'unique_id': layout.get('uniqueId', ''),
                   'markdown_content': layout.get('markdownContent', '')  # æ–°å¢Markdownå†…å®¹
               }

               # æ ¹æ®å†…å®¹é•¿åº¦å†³å®šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†
               if len(chunk['content']) > self.max_chunk_size:
                   sub_chunks = self._split_large_content(chunk)
                   chunks.extend(sub_chunks)
               else:
                   chunks.append(chunk)

           return chunks

       def _extract_title(self, layout):
           """ä»layoutä¸­æå–æ ‡é¢˜"""
           if layout.get('type') in ['title', 'para_title']:
               return layout.get('text', '')
           return ''

       def _split_large_content(self, chunk):
           """åˆ‡åˆ†è¿‡é•¿çš„å†…å®¹"""
           content = chunk['content']
           sub_chunks = []

           for i in range(0, len(content), self.max_chunk_size - self.overlap):
               sub_content = content[i:i + self.max_chunk_size]
               sub_chunk = chunk.copy()
               sub_chunk['content'] = sub_content
               sub_chunks.append(sub_chunk)

           return sub_chunks

       def extract_metadata(self, chunk, document):
           """æå–æ–‡æ¡£ç‰‡æ®µçš„å…ƒæ•°æ®"""
           return {
               'vendor': self._detect_vendor(chunk['content']),
               'category': self._classify_content(chunk['content']),
               'source_document': document.original_filename,
               'page_number': chunk.get('page_number', 0),
               'element_type': chunk.get('type', 'text'),
               'element_subtype': chunk.get('subtype', ''),
               'unique_id': chunk.get('unique_id', ''),
               'has_markdown': bool(chunk.get('markdown_content'))
           }

       def _detect_vendor(self, content):
           """æ£€æµ‹å‚å•†ä¿¡æ¯"""
           content_lower = content.lower()
           if any(keyword in content_lower for keyword in ['åä¸º', 'huawei', 'vrp']):
               return 'åä¸º'
           elif any(keyword in content_lower for keyword in ['æ€ç§‘', 'cisco', 'ios']):
               return 'æ€ç§‘'
           elif any(keyword in content_lower for keyword in ['åä¸‰', 'h3c', 'comware']):
               return 'åä¸‰'
           return 'é€šç”¨'

       def _classify_content(self, content):
           """åˆ†ç±»å†…å®¹"""
           content_lower = content.lower()
           if any(keyword in content_lower for keyword in ['ospf', 'open shortest path first']):
               return 'OSPF'
           elif any(keyword in content_lower for keyword in ['bgp', 'border gateway protocol']):
               return 'BGP'
           elif any(keyword in content_lower for keyword in ['mpls', 'multiprotocol label switching']):
               return 'MPLS'
           elif any(keyword in content_lower for keyword in ['vpn', 'virtual private network']):
               return 'VPN'
           return 'å…¶ä»–'
   ```

**éªŒæ”¶æ ‡å‡†**:
- IDPæœåŠ¡é›†æˆåˆ°åç«¯å¼‚æ­¥ä»»åŠ¡ç³»ç»Ÿ
- è¯­ä¹‰åˆ‡åˆ†ç®—æ³•å®ç°å®Œæˆ
- èƒ½å¤„ç†PDFã€Wordã€å›¾ç‰‡ç­‰å¤šç§æ ¼å¼

### 7.3 æµ‹è¯•IDPé›†æˆ (ç¬¬3å¤©)

**ç›®æ ‡**: æµ‹è¯•æ–‡æ¡£è§£æå’Œåˆ‡åˆ†åŠŸèƒ½ã€‚

**å…·ä½“æ­¥éª¤**:

1. **ç¼–å†™æµ‹è¯•ç”¨ä¾‹**
   ```python
   def test_idp_integration():
       # æµ‹è¯•ä¸åŒç±»å‹æ–‡æ¡£è§£æ
       # æµ‹è¯•è¯­ä¹‰åˆ‡åˆ†æ•ˆæœ
       # æµ‹è¯•å…ƒæ•°æ®æå–
       pass
   ```

2. **æ€§èƒ½æµ‹è¯•**
   - æµ‹è¯•è§£æé€Ÿåº¦
   - æµ‹è¯•å¹¶å‘å¤„ç†èƒ½åŠ›
   - æµ‹è¯•é”™è¯¯å¤„ç†æœºåˆ¶

**éªŒæ”¶æ ‡å‡†**:
- æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡
- è§£æå‡†ç¡®ç‡>90%
- å¹³å‡è§£ææ—¶é—´<30ç§’/æ–‡æ¡£

---

## ğŸ¯ ä»»åŠ¡8: å‘é‡æ•°æ®åº“æ„å»ºä¸é…ç½®

### 8.1 Weaviateæœ¬åœ°éƒ¨ç½² (ç¬¬1å¤©)

**ç›®æ ‡**: ä½¿ç”¨Dockeråœ¨æœ¬åœ°éƒ¨ç½²Weaviateå‘é‡æ•°æ®åº“ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿä¸»å¯¼ï¼Œåç«¯å›¢é˜ŸååŠ©Dockeré…ç½®

**å…·ä½“æ­¥éª¤**:

1. **æ›´æ–°Docker Composeé…ç½®** (`docker-compose.local.yml`)
   ```yaml
   version: '3.8'
   services:
     weaviate:
       image: semitechnologies/weaviate:1.21.2
       ports:
         - "8080:8080"
         - "50051:50051"
       environment:
         QUERY_DEFAULTS_LIMIT: 25
         AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
         PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
         DEFAULT_VECTORIZER_MODULE: 'none'
         ENABLE_MODULES: 'text2vec-openai,generative-openai'
         CLUSTER_HOSTNAME: 'node1'
       volumes:
         - weaviate_data:/var/lib/weaviate

     mysql:
       image: mysql:8.0
       ports:
         - "3306:3306"
       environment:
         MYSQL_ROOT_PASSWORD: password
         MYSQL_DATABASE: ip_expert
       volumes:
         - mysql_data:/var/lib/mysql

     redis:
       image: redis:7-alpine
       ports:
         - "6379:6379"
       volumes:
         - redis_data:/data

   volumes:
     weaviate_data:
     mysql_data:
     redis_data:
   ```

2. **åˆ›å»ºå‘é‡æœåŠ¡** (`app/services/vector_service.py`)
   ```python
   import weaviate
   from langchain_community.embeddings import DashScopeEmbeddings

   class VectorService:
       def __init__(self):
           self.client = weaviate.Client("http://localhost:8080")
           # ä½¿ç”¨Langchainé›†æˆçš„DashScopeå‘é‡æ¨¡å‹
           self.embedding_service = DashScopeEmbeddings(
               model="text-embedding-v4"
           )

       def create_schema(self):
           schema = {
               "class": "KnowledgeChunk",
               "description": "ç½‘ç»œè¿ç»´çŸ¥è¯†ç‰‡æ®µ",
               "properties": [
                   {"name": "content", "dataType": ["text"]},
                   {"name": "title", "dataType": ["string"]},
                   {"name": "vendor", "dataType": ["string"]},
                   {"name": "category", "dataType": ["string"]},
                   {"name": "source_document", "dataType": ["string"]},
                   {"name": "page_number", "dataType": ["int"]}
               ],
               "vectorizer": "none"
           }
           self.client.schema.create_class(schema)

       def index_chunks(self, chunks, document_id):
           """æ‰¹é‡ç´¢å¼•æ–‡æ¡£ç‰‡æ®µ"""
           with self.client.batch as batch:
               for chunk in chunks:
                   vector = self.embedding_service.embed_text(chunk['content'])

                   batch.add_data_object(
                       data_object=chunk,
                       class_name="KnowledgeChunk",
                       vector=vector
                   )
   ```

**éªŒæ”¶æ ‡å‡†**:
- WeaviateæœåŠ¡æ­£å¸¸å¯åŠ¨
- Schemaåˆ›å»ºæˆåŠŸ
- æ”¯æŒå‘é‡ç´¢å¼•å’ŒæŸ¥è¯¢

### 8.2 å‘é‡åŒ–æ¨¡å‹é›†æˆ (ç¬¬2å¤©)

**ç›®æ ‡**: é›†æˆé˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°çš„text-embedding-v4æ¨¡å‹ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿ

**å…·ä½“æ­¥éª¤**:

1. **åˆ›å»ºåµŒå…¥æœåŠ¡** (`app/services/embedding_service.py`)
   ```python
   from langchain_community.embeddings import DashScopeEmbeddings
   from typing import List
   import os

   class QwenEmbedding:
       def __init__(self, api_key=None):
           """ä½¿ç”¨Langchainé›†æˆçš„DashScopeå‘é‡æ¨¡å‹"""
           self.api_key = api_key or os.environ.get('DASHSCOPE_API_KEY')
           self.embeddings = DashScopeEmbeddings(
               model="text-embedding-v4",
               dashscope_api_key=self.api_key
           )

       def embed_text(self, text: str) -> List[float]:
           """å•ä¸ªæ–‡æœ¬å‘é‡åŒ–"""
           try:
               return self.embeddings.embed_query(text)
           except Exception as e:
               raise Exception(f"å‘é‡åŒ–å¤±è´¥: {str(e)}")

       def embed_batch(self, texts: List[str]) -> List[List[float]]:
           """æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–"""
           try:
               return self.embeddings.embed_documents(texts)
           except Exception as e:
               # é™çº§åˆ°å•ä¸ªå¤„ç†
               embeddings = []
               for text in texts:
                   try:
                       emb = self.embed_text(text)
                       embeddings.append(emb)
                   except:
                       embeddings.append([0.0] * 1536)  # é»˜è®¤ç»´åº¦
               return embeddings
   ```

**éªŒæ”¶æ ‡å‡†**:
- èƒ½æˆåŠŸè°ƒç”¨text-embedding-v4 API
- æ”¯æŒæ‰¹é‡å‘é‡åŒ–å¤„ç†
- å‘é‡ç»´åº¦æ­£ç¡®(1536ç»´)

---

## ğŸ¯ ä»»åŠ¡9: æ··åˆæ£€ç´¢ç®—æ³•å®ç°

### 9.1 ä¸¤é˜¶æ®µæ£€ç´¢å®ç° (ç¬¬1-2å¤©)

**ç›®æ ‡**: å®ç°å¬å›+ç²¾æ’çš„ä¸¤é˜¶æ®µæ£€ç´¢ç®—æ³•ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿä¸»å¯¼ï¼Œåç«¯å›¢é˜Ÿé…åˆAPIé›†æˆ

**å…·ä½“æ­¥éª¤**:

1. **åˆ›å»ºæ£€ç´¢æœåŠ¡** (`app/services/retrieval_service.py`)
   ```python
   import weaviate
   from langchain_community.embeddings import DashScopeEmbeddings
   from typing import List, Dict

   class RetrievalService:
       def __init__(self):
           self.client = weaviate.Client("http://localhost:8080")
           # ä½¿ç”¨Langchainé›†æˆçš„DashScopeå‘é‡æ¨¡å‹
           self.embedding_service = DashScopeEmbeddings(
               model="text-embedding-v4"
           )

       def hybrid_search(self, query: str, top_k: int = 5,
                        vendor_filter: str = None) -> List[Dict]:
           """æ··åˆæ£€ç´¢ï¼šå¬å›+ç²¾æ’"""

           # ç¬¬ä¸€é˜¶æ®µï¼šå¬å›
           candidates = self._recall_stage(query, vendor_filter)

           # ç¬¬äºŒé˜¶æ®µï¼šç²¾æ’
           ranked_results = self._rerank_stage(query, candidates, top_k)

           return ranked_results

       def _recall_stage(self, query: str, vendor_filter: str = None) -> List[Dict]:
           """å¬å›é˜¶æ®µï¼šBM25 + å‘é‡æ£€ç´¢"""

           # BM25å…³é”®è¯æ£€ç´¢
           bm25_results = self._bm25_search(query, vendor_filter, limit=50)

           # å‘é‡è¯­ä¹‰æ£€ç´¢
           vector_results = self._vector_search(query, vendor_filter, limit=50)

           # åˆå¹¶å»é‡
           combined_results = self._merge_results(bm25_results, vector_results)

           return combined_results

       def _bm25_search(self, query: str, vendor_filter: str = None, limit: int = 50):
           """BM25å…³é”®è¯æ£€ç´¢"""
           where_filter = None
           if vendor_filter:
               where_filter = {"path": ["vendor"], "operator": "Equal", "valueString": vendor_filter}

           result = self.client.query.get("KnowledgeChunk", [
               "content", "title", "vendor", "category", "source_document", "page_number"
           ]).with_bm25(
               query=query
           ).with_where(where_filter).with_limit(limit).do()

           return result.get('data', {}).get('Get', {}).get('KnowledgeChunk', [])

       def _vector_search(self, query: str, vendor_filter: str = None, limit: int = 50):
           """å‘é‡è¯­ä¹‰æ£€ç´¢"""
           query_vector = self.embedding_service.embed_text(query)

           where_filter = None
           if vendor_filter:
               where_filter = {"path": ["vendor"], "operator": "Equal", "valueString": vendor_filter}

           result = self.client.query.get("KnowledgeChunk", [
               "content", "title", "vendor", "category", "source_document", "page_number"
           ]).with_near_vector({
               "vector": query_vector
           }).with_where(where_filter).with_limit(limit).do()

           return result.get('data', {}).get('Get', {}).get('KnowledgeChunk', [])

       def _rerank_stage(self, query: str, candidates: List[Dict], top_k: int) -> List[Dict]:
           """ç²¾æ’é˜¶æ®µï¼šä½¿ç”¨Qwen3-Reranker"""
           if not candidates:
               return []

           # è°ƒç”¨é‡æ’åºæ¨¡å‹
           scores = self._call_reranker(query, candidates)

           # æŒ‰åˆ†æ•°æ’åº
           scored_candidates = list(zip(candidates, scores))
           scored_candidates.sort(key=lambda x: x[1], reverse=True)

           # è¿”å›top_kç»“æœ
           return [candidate for candidate, score in scored_candidates[:top_k]]

       def _call_reranker(self, query: str, candidates: List[Dict]) -> List[float]:
           """è°ƒç”¨OLLAMAéƒ¨ç½²çš„Qwen3-Rerankeræ¨¡å‹"""
           # TODO: å®ç°OLLAMA APIè°ƒç”¨
           # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿåˆ†æ•°
           import random
           return [random.random() for _ in candidates]
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ··åˆæ£€ç´¢åŠŸèƒ½æ­£å¸¸
- å¬å›ç‡>80%
- æ£€ç´¢é€Ÿåº¦<500ms

### 9.2 é‡æ’åºæ¨¡å‹éƒ¨ç½² (ç¬¬3å¤©)

**ç›®æ ‡**: ä½¿ç”¨OLLAMAéƒ¨ç½²Qwen3-Rerankeræ¨¡å‹ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿ

**å…·ä½“æ­¥éª¤**:

1. **å®‰è£…OLLAMA**
   ```bash
   # macOS
   brew install ollama

   # Linux
   curl -fsSL https://ollama.ai/install.sh | sh
   ```

2. **éƒ¨ç½²é‡æ’åºæ¨¡å‹**
   ```bash
   ollama pull qwen3-reranker
   ollama serve
   ```

3. **é›†æˆé‡æ’åºAPI** (`app/services/reranker_service.py`)
   ```python
   from langchain_community.document_compressors import LLMChainExtractor
   from langchain_community.llms import Ollama
   from langchain.schema import Document
   from typing import List, Dict
   import os

   class RerankerService:
       def __init__(self, ollama_url=None):
           """ä½¿ç”¨Langchainé›†æˆOLLAMAé‡æ’åºæ¨¡å‹"""
           self.ollama_url = ollama_url or os.environ.get('OLLAMA_BASE_URL', 'http://localhost:11434')

           # ä½¿ç”¨Langchainçš„Ollamaé›†æˆ
           self.llm = Ollama(
               model="qwen3-reranker",
               base_url=self.ollama_url
           )

       def rerank(self, query: str, documents: List[str]) -> List[float]:
           """ä½¿ç”¨Qwen3-Rerankerå¯¹æ–‡æ¡£é‡æ’åº"""
           scores = []

           for doc in documents:
               # æ„å»ºé‡æ’åºæç¤ºè¯
               prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼Œè¿”å›0-1ä¹‹é—´çš„åˆ†æ•°ï¼š

æŸ¥è¯¢: {query}

æ–‡æ¡£: {doc}

ç›¸å…³æ€§åˆ†æ•°:"""

               try:
                   response = self.llm.invoke(prompt)
                   score = self._parse_score(response)
                   scores.append(score)
               except Exception as e:
                   print(f"é‡æ’åºå¤±è´¥: {e}")
                   scores.append(0.5)  # é»˜è®¤åˆ†æ•°

           return scores

       def _parse_score(self, response: str) -> float:
           """è§£ææ¨¡å‹è¿”å›çš„ç›¸å…³æ€§åˆ†æ•°"""
           try:
               import re
               # æŸ¥æ‰¾0-1ä¹‹é—´çš„å°æ•°
               match = re.search(r'0\.\d+|1\.0|0|1', response)
               if match:
                   score = float(match.group())
                   return max(0.0, min(1.0, score))  # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
           except:
               pass
           return 0.5  # é»˜è®¤åˆ†æ•°
   ```

**éªŒæ”¶æ ‡å‡†**:
- OLLAMAæœåŠ¡æ­£å¸¸è¿è¡Œ
- é‡æ’åºæ¨¡å‹éƒ¨ç½²æˆåŠŸ
- APIè°ƒç”¨æ­£å¸¸

---

## ğŸ¯ ä»»åŠ¡10: æç¤ºè¯å·¥ç¨‹ä¸ä¼˜åŒ–

### 10.1 åŸºç¡€Promptè®¾è®¡ (ç¬¬1-2å¤©)

**ç›®æ ‡**: è®¾è®¡ç”¨äºä¸åŒåœºæ™¯çš„åŸºç¡€æç¤ºè¯æ¨¡æ¿ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿä¸»å¯¼

**å…·ä½“æ­¥éª¤**:

1. **åˆ›å»ºPromptæ¨¡æ¿åº“** (`app/prompts/`)
   ```python
   # app/prompts/analysis_prompt.py
   ANALYSIS_PROMPT = """
   ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç½‘ç»œè¿ç»´ä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·æå‡ºçš„ç½‘ç»œé—®é¢˜ã€‚

   ç”¨æˆ·é—®é¢˜: {user_query}

   ä¸Šä¸‹æ–‡ä¿¡æ¯:
   {context}

   è¯·æŒ‰ä»¥ä¸‹æ ¼å¼åˆ†æ:
   1. é—®é¢˜åˆ†ç±»: [OSPF/BGP/MPLS/VPN/å…¶ä»–]
   2. å¯èƒ½åŸå› : åˆ—å‡º3-5ä¸ªå¯èƒ½çš„åŸå› 
   3. éœ€è¦è¡¥å……çš„ä¿¡æ¯: å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œåˆ—å‡ºéœ€è¦ç”¨æˆ·æä¾›çš„å…·ä½“ä¿¡æ¯
   4. åˆæ­¥å»ºè®®: ç»™å‡ºåˆæ­¥çš„æ’æŸ¥æ–¹å‘

   å¦‚æœä¿¡æ¯ä¸è¶³ä»¥ç»™å‡ºå®Œæ•´åˆ†æï¼Œè¯·åœ¨å›ç­”æœ«å°¾æ·»åŠ : [NEED_MORE_INFO]
   """

   CLARIFICATION_PROMPT = """
   åŸºäºå½“å‰çš„åˆ†æï¼Œæˆ‘éœ€è¦æ›´å¤šä¿¡æ¯æ¥å¸®åŠ©æ‚¨è§£å†³é—®é¢˜ã€‚

   å½“å‰åˆ†æ: {current_analysis}

   è¯·å›ç­”ä»¥ä¸‹é—®é¢˜:
   {questions}

   æ‚¨ä¹Ÿå¯ä»¥ä¸Šä¼ ç›¸å…³çš„é…ç½®æ–‡ä»¶ã€æ—¥å¿—æ–‡ä»¶æˆ–ç½‘ç»œæ‹“æ‰‘å›¾ã€‚
   """

   SOLUTION_PROMPT = """
   ä½ æ˜¯ä¸€ä½{vendor}ç½‘ç»œè®¾å¤‡ä¸“å®¶ã€‚åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œæä¾›è¯¦ç»†çš„è§£å†³æ–¹æ¡ˆã€‚

   é—®é¢˜æè¿°: {problem}
   ç›¸å…³æ–‡æ¡£: {retrieved_docs}
   ç”¨æˆ·è¡¥å……ä¿¡æ¯: {user_context}

   è¯·æä¾›:
   1. é—®é¢˜æ ¹å› åˆ†æ
   2. è¯¦ç»†è§£å†³æ­¥éª¤ (æ¯æ­¥éƒ½è¦å…·ä½“å¯æ‰§è¡Œ)
   3. éªŒè¯æ–¹æ³•
   4. é¢„é˜²æªæ–½

   åœ¨å¼•ç”¨æ–‡æ¡£å†…å®¹æ—¶ï¼Œè¯·ä½¿ç”¨ [doc1], [doc2] ç­‰æ ‡è®°ã€‚
   æ‰€æœ‰å‘½ä»¤éƒ½è¦ä½¿ç”¨{vendor}çš„è¯­æ³•æ ¼å¼ã€‚
   """
   ```

2. **å‚å•†é€‚é…Prompt**
   ```python
   # app/prompts/vendor_prompts.py
   HUAWEI_PROMPT = """
   ä½ æ˜¯åä¸ºVRPç³»ç»Ÿä¸“å®¶ã€‚è¯·ä½¿ç”¨åä¸ºè®¾å¤‡çš„å‘½ä»¤è¯­æ³•å›ç­”é—®é¢˜ã€‚

   å¸¸ç”¨å‘½ä»¤æ ¼å¼:
   - æŸ¥çœ‹æ¥å£: display interface
   - æŸ¥çœ‹è·¯ç”±: display ip routing-table
   - æŸ¥çœ‹OSPF: display ospf peer
   - é…ç½®æ¥å£: interface GigabitEthernet0/0/1

   {base_prompt}

   æ³¨æ„: æ‰€æœ‰å‘½ä»¤å¿…é¡»ç¬¦åˆVRPè¯­æ³•è§„èŒƒã€‚
   """

   CISCO_PROMPT = """
   ä½ æ˜¯æ€ç§‘IOSç³»ç»Ÿä¸“å®¶ã€‚è¯·ä½¿ç”¨æ€ç§‘è®¾å¤‡çš„å‘½ä»¤è¯­æ³•å›ç­”é—®é¢˜ã€‚

   å¸¸ç”¨å‘½ä»¤æ ¼å¼:
   - æŸ¥çœ‹æ¥å£: show interfaces
   - æŸ¥çœ‹è·¯ç”±: show ip route
   - æŸ¥çœ‹OSPF: show ip ospf neighbor
   - é…ç½®æ¥å£: interface GigabitEthernet0/0

   {base_prompt}

   æ³¨æ„: æ‰€æœ‰å‘½ä»¤å¿…é¡»ç¬¦åˆIOSè¯­æ³•è§„èŒƒã€‚
   """
   ```

**éªŒæ”¶æ ‡å‡†**:
- Promptæ¨¡æ¿è¦†ç›–æ‰€æœ‰èŠ‚ç‚¹ç±»å‹
- æ”¯æŒå‚å•†é€‚é…
- è¾“å‡ºæ ¼å¼è§„èŒƒç»Ÿä¸€

### 10.2 LLMæœåŠ¡é›†æˆ (ç¬¬3å¤©)

**ç›®æ ‡**: é›†æˆé˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°çš„qwen-plusæ¨¡å‹ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿä¸»å¯¼ï¼Œåç«¯å›¢é˜Ÿé…åˆ

**å…·ä½“æ­¥éª¤**:

1. **åˆ›å»ºLLMæœåŠ¡** (`app/services/llm_service.py`)
   ```python
   from langchain_openai import ChatOpenAI
   from langchain.schema import HumanMessage, SystemMessage
   from app.prompts.analysis_prompt import ANALYSIS_PROMPT, SOLUTION_PROMPT
   import os

   class LLMService:
       def __init__(self):
           self.llm = ChatOpenAI(
               model="qwen-plus",
               openai_api_key=os.environ.get('DASHSCOPE_API_KEY'),
               openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
               temperature=0.1
           )

       def analyze_query(self, query: str, context: str = "") -> dict:
           """åˆ†æç”¨æˆ·æŸ¥è¯¢"""
           prompt = ANALYSIS_PROMPT.format(
               user_query=query,
               context=context
           )

           messages = [
               SystemMessage(content="ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç½‘ç»œè¿ç»´ä¸“å®¶ã€‚"),
               HumanMessage(content=prompt)
           ]

           response = self.llm.invoke(messages)

           # è§£æå“åº”
           need_more_info = "[NEED_MORE_INFO]" in response.content

           return {
               "analysis": response.content,
               "need_more_info": need_more_info,
               "category": self._extract_category(response.content)
           }

       def generate_solution(self, query: str, context: list,
                           vendor: str = "é€šç”¨") -> dict:
           """ç”Ÿæˆè§£å†³æ–¹æ¡ˆ"""
           retrieved_docs = self._format_context(context)

           prompt = SOLUTION_PROMPT.format(
               vendor=vendor,
               problem=query,
               retrieved_docs=retrieved_docs,
               user_context=""
           )

           messages = [
               SystemMessage(content=f"ä½ æ˜¯ä¸€ä½{vendor}ç½‘ç»œè®¾å¤‡ä¸“å®¶ã€‚"),
               HumanMessage(content=prompt)
           ]

           response = self.llm.invoke(messages)

           return {
               "answer": response.content,
               "sources": self._extract_sources(context),
               "commands": self._extract_commands(response.content)
           }

       def _extract_category(self, text: str) -> str:
           """ä»åˆ†æç»“æœä¸­æå–é—®é¢˜åˆ†ç±»"""
           # ç®€å•çš„å…³é”®è¯åŒ¹é…
           if "OSPF" in text:
               return "OSPF"
           elif "BGP" in text:
               return "BGP"
           elif "MPLS" in text:
               return "MPLS"
           else:
               return "å…¶ä»–"

       def _format_context(self, context: list) -> str:
           """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡"""
           formatted = ""
           for i, doc in enumerate(context, 1):
               formatted += f"[doc{i}] {doc.get('title', '')}\n{doc.get('content', '')}\n\n"
           return formatted

       def _extract_sources(self, context: list) -> list:
           """æå–å¼•ç”¨æ¥æº"""
           sources = []
           for i, doc in enumerate(context, 1):
               sources.append({
                   "id": f"doc{i}",
                   "file": doc.get('source_document', ''),
                   "page": doc.get('page_number', 0)
               })
           return sources

       def _extract_commands(self, text: str) -> list:
           """ä»å›ç­”ä¸­æå–å‘½ä»¤"""
           import re
           # ç®€å•çš„å‘½ä»¤æå–é€»è¾‘
           commands = re.findall(r'`([^`]+)`', text)
           return commands
   ```

**éªŒæ”¶æ ‡å‡†**:
- LLMæœåŠ¡é›†æˆæˆåŠŸ
- æ”¯æŒæµå¼å’Œéæµå¼è¾“å‡º
- é”™è¯¯å¤„ç†å®Œå–„

---

## ğŸ¯ ä»»åŠ¡11: langgraph Agentæµç¨‹æ„å»º

### 11.1 çŠ¶æ€æœºè®¾è®¡ (ç¬¬1-2å¤©)

**ç›®æ ‡**: è®¾è®¡å¤šè½®å¯¹è¯çš„çŠ¶æ€æœºæ¶æ„ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿä¸»å¯¼ï¼Œåç«¯å›¢é˜Ÿé…åˆé›†æˆ

**å…·ä½“æ­¥éª¤**:

1. **å®šä¹‰AgentçŠ¶æ€** (`app/services/agent_state.py`)
   ```python
   from typing import TypedDict, List, Dict, Optional

   class AgentState(TypedDict):
       messages: List[Dict]      # å¯¹è¯å†å²
       context: List[Dict]       # æ£€ç´¢åˆ°çš„çŸ¥è¯†
       user_query: str          # ç”¨æˆ·é—®é¢˜
       vendor: Optional[str]    # è®¾å¤‡å‚å•†
       category: Optional[str]  # é—®é¢˜åˆ†ç±»
       need_more_info: bool     # æ˜¯å¦éœ€è¦æ›´å¤šä¿¡æ¯
       solution_ready: bool     # æ˜¯å¦å¯ä»¥ç»™å‡ºè§£å†³æ–¹æ¡ˆ
       case_id: str            # æ¡ˆä¾‹ID
       current_node_id: str    # å½“å‰èŠ‚ç‚¹ID
   ```

2. **å®ç°AgentèŠ‚ç‚¹** (`app/services/agent_nodes.py`)
   ```python
   from app.services.llm_service import LLMService
   from app.services.retrieval_service import RetrievalService
   from app.models.case import Node
   from app import db

   def analyze_query(state: AgentState) -> AgentState:
       """åˆ†æç”¨æˆ·é—®é¢˜"""
       llm_service = LLMService()

       analysis_result = llm_service.analyze_query(
           state["user_query"],
           context=""
       )

       # æ›´æ–°çŠ¶æ€
       state["category"] = analysis_result.get("category")
       state["need_more_info"] = analysis_result.get("need_more_info", False)

       # æ›´æ–°æ•°æ®åº“èŠ‚ç‚¹
       node = Node.query.get(state["current_node_id"])
       if node:
           node.content = {
               "analysis": analysis_result.get("analysis"),
               "category": analysis_result.get("category")
           }
           if state["need_more_info"]:
               node.type = "AI_CLARIFICATION"
               node.status = "AWAITING_USER_INPUT"
           else:
               node.type = "AI_ANALYSIS"
               node.status = "COMPLETED"
           db.session.commit()

       return state

   def retrieve_knowledge(state: AgentState) -> AgentState:
       """æ£€ç´¢ç›¸å…³çŸ¥è¯†"""
       retrieval_service = RetrievalService()

       context = retrieval_service.hybrid_search(
           query=state["user_query"],
           vendor_filter=state.get("vendor")
       )

       state["context"] = context
       return state

   def generate_solution(state: AgentState) -> AgentState:
       """ç”Ÿæˆè§£å†³æ–¹æ¡ˆ"""
       llm_service = LLMService()

       solution = llm_service.generate_solution(
           query=state["user_query"],
           context=state["context"],
           vendor=state.get("vendor", "é€šç”¨")
       )

       # æ›´æ–°æ•°æ®åº“èŠ‚ç‚¹
       node = Node.query.get(state["current_node_id"])
       if node:
           node.type = "SOLUTION"
           node.title = "è§£å†³æ–¹æ¡ˆ"
           node.status = "COMPLETED"
           node.content = {
               "answer": solution.get("answer"),
               "sources": solution.get("sources"),
               "commands": solution.get("commands")
           }
           db.session.commit()

       state["solution_ready"] = True
       return state
   ```

**éªŒæ”¶æ ‡å‡†**:
- çŠ¶æ€ç»“æ„è®¾è®¡åˆç†
- èŠ‚ç‚¹åŠŸèƒ½å®ç°å®Œæ•´
- ä¸æ•°æ®åº“é›†æˆæ­£å¸¸

### 11.2 å·¥ä½œæµç¼–æ’ (ç¬¬3-4å¤©)

**ç›®æ ‡**: ä½¿ç”¨langgraphæ„å»ºå®Œæ•´çš„å¯¹è¯æµç¨‹ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿä¸»å¯¼

**å…·ä½“æ­¥éª¤**:

1. **åˆ›å»ºAgentå·¥ä½œæµ** (`app/services/agent_workflow.py`)
   ```python
   from langgraph.graph import StateGraph, END
   from app.services.agent_state import AgentState
   from app.services.agent_nodes import analyze_query, retrieve_knowledge, generate_solution

   def create_agent_workflow():
       """åˆ›å»ºAgentå·¥ä½œæµ"""
       workflow = StateGraph(AgentState)

       # æ·»åŠ èŠ‚ç‚¹
       workflow.add_node("analyze", analyze_query)
       workflow.add_node("retrieve", retrieve_knowledge)
       workflow.add_node("solve", generate_solution)

       # è®¾ç½®å…¥å£ç‚¹
       workflow.set_entry_point("analyze")

       # å®šä¹‰æ¡ä»¶è¾¹
       workflow.add_conditional_edges(
           "analyze",
           should_retrieve_or_end,
           {
               "retrieve": "retrieve",
               "end": END
           }
       )

       workflow.add_edge("retrieve", "solve")
       workflow.add_edge("solve", END)

       return workflow.compile()

   def should_retrieve_or_end(state: AgentState) -> str:
       """åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢çŸ¥è¯†"""
       if state.get("need_more_info"):
           return "end"  # éœ€è¦æ›´å¤šä¿¡æ¯ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥
       return "retrieve"
   ```

2. **é›†æˆåˆ°å¼‚æ­¥ä»»åŠ¡** (æ›´æ–° `app/services/agent_service.py`)
   ```python
   from app.services.agent_workflow import create_agent_workflow

   def analyze_user_query(case_id, node_id, query):
       """åˆ†æç”¨æˆ·æŸ¥è¯¢çš„å¼‚æ­¥ä»»åŠ¡"""
       app = create_app()
       with app.app_context():
           try:
               # åˆ›å»ºAgentå·¥ä½œæµ
               agent = create_agent_workflow()

               # åˆå§‹åŒ–çŠ¶æ€
               initial_state = {
                   "user_query": query,
                   "messages": [],
                   "context": [],
                   "need_more_info": False,
                   "solution_ready": False,
                   "case_id": case_id,
                   "current_node_id": node_id
               }

               # æ‰§è¡Œå·¥ä½œæµ
               final_state = agent.invoke(initial_state)

           except Exception as e:
               # é”™è¯¯å¤„ç†
               node = Node.query.get(node_id)
               if node:
                   node.status = "COMPLETED"
                   node.content = {"error": "å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œè¯·é‡è¯•"}
                   db.session.commit()
               raise
   ```

**éªŒæ”¶æ ‡å‡†**:
- å·¥ä½œæµé€»è¾‘æ­£ç¡®
- ä¸åç«¯å¼‚æ­¥ä»»åŠ¡é›†æˆ
- æ”¯æŒå¤šè½®å¯¹è¯

---

## ğŸ¯ ä»»åŠ¡12: æ¨¡å‹æ¥å£é›†æˆä¸æµ‹è¯•

### 12.1 æ€§èƒ½ä¼˜åŒ– (ç¬¬1å¤©)

**ç›®æ ‡**: ä¼˜åŒ–æ¨¡å‹è°ƒç”¨æ€§èƒ½ï¼Œæ·»åŠ ç›‘æ§ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿ

**å…·ä½“æ­¥éª¤**:

1. **æ·»åŠ ç¼“å­˜æœºåˆ¶**
   ```python
   # app/services/cache_service.py
   import redis
   import hashlib
   import json

   class CacheService:
       def __init__(self):
           self.redis_client = redis.from_url(os.environ.get('REDIS_URL'))

       def get_cached_result(self, key: str):
           """è·å–ç¼“å­˜ç»“æœ"""
           cached = self.redis_client.get(key)
           if cached:
               return json.loads(cached)
           return None

       def cache_result(self, key: str, result: dict, expire_time: int = 3600):
           """ç¼“å­˜ç»“æœ"""
           self.redis_client.setex(key, expire_time, json.dumps(result))

       def generate_cache_key(self, prefix: str, *args) -> str:
           """ç”Ÿæˆç¼“å­˜é”®"""
           content = f"{prefix}:{':'.join(map(str, args))}"
           return hashlib.md5(content.encode()).hexdigest()
   ```

2. **æ·»åŠ æ€§èƒ½ç›‘æ§**
   ```python
   # app/utils/monitoring.py
   import time
   import logging
   from functools import wraps

   def monitor_performance(operation_name: str):
       def decorator(func):
           @wraps(func)
           def wrapper(*args, **kwargs):
               start_time = time.time()
               try:
                   result = func(*args, **kwargs)
                   duration = time.time() - start_time
                   logging.info(f"{operation_name} success: {duration:.2f}s")
                   return result
               except Exception as e:
                   duration = time.time() - start_time
                   logging.error(f"{operation_name} failed: {duration:.2f}s, {e}")
                   raise
           return wrapper
       return decorator
   ```

**éªŒæ”¶æ ‡å‡†**:
- ç¼“å­˜æœºåˆ¶å·¥ä½œæ­£å¸¸
- æ€§èƒ½ç›‘æ§æ•°æ®å‡†ç¡®
- å¹³å‡å“åº”æ—¶é—´<3ç§’

### 12.2 é›†æˆæµ‹è¯• (ç¬¬2å¤©)

**ç›®æ ‡**: æµ‹è¯•æ‰€æœ‰AIç»„ä»¶çš„é›†æˆæ•ˆæœã€‚

**è´Ÿè´£äºº**: å…¨å›¢é˜Ÿ

**å…·ä½“æ­¥éª¤**:

1. **ç«¯åˆ°ç«¯æµ‹è¯•**
   ```python
   def test_full_ai_pipeline():
       """æµ‹è¯•å®Œæ•´çš„AIæµç¨‹"""
       # æµ‹è¯•æ–‡æ¡£ä¸Šä¼ å’Œè§£æ
       # æµ‹è¯•å‘é‡åŒ–å’Œç´¢å¼•
       # æµ‹è¯•æ£€ç´¢å’Œé‡æ’åº
       # æµ‹è¯•Agentå¯¹è¯æµç¨‹
       pass
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ‰€æœ‰AIç»„ä»¶é›†æˆæµ‹è¯•é€šè¿‡
- ç«¯åˆ°ç«¯æµç¨‹æ­£å¸¸
- æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡

---

## ğŸ¯ ä»»åŠ¡13: ç³»ç»Ÿé›†æˆä¸ç«¯åˆ°ç«¯æµ‹è¯•

### 13.1 APIé›†æˆæµ‹è¯• (ç¬¬1-2å¤©)

**ç›®æ ‡**: è¿›è¡Œå®Œæ•´çš„APIé›†æˆæµ‹è¯•ã€‚

**è´Ÿè´£äºº**: å…¨å›¢é˜Ÿåä½œ

**å…·ä½“æ­¥éª¤**:

1. **æµ‹è¯•ç”¨ä¾‹ç¼–å†™** (`tests/test_api.py`)
   ```python
   import pytest
   from app import create_app, db
   from app.models.user import User

   @pytest.fixture
   def app():
       app = create_app('testing')
       with app.app_context():
           db.create_all()
           yield app
           db.drop_all()

   @pytest.fixture
   def client(app):
       return app.test_client()

   @pytest.fixture
   def auth_headers(client):
       # åˆ›å»ºæµ‹è¯•ç”¨æˆ·å¹¶è·å–token
       user = User(username='test', email='test@example.com')
       user.set_password('password')
       db.session.add(user)
       db.session.commit()

       response = client.post('/api/v1/auth/login', json={
           'username': 'test',
           'password': 'password'
       })
       token = response.get_json()['access_token']
       return {'Authorization': f'Bearer {token}'}

   def test_create_case(client, auth_headers):
       response = client.post('/api/v1/cases',
           headers=auth_headers,
           json={
               'query': 'OSPFé‚»å±…å»ºç«‹å¤±è´¥',
               'attachments': []
           }
       )
       assert response.status_code == 200
       data = response.get_json()
       assert 'caseId' in data
       assert len(data['nodes']) == 2
   ```

2. **è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬**
   ```bash
   #!/bin/bash
   # run_tests.sh

   echo "Running API tests..."
   pytest tests/test_api.py -v

   echo "Running integration tests..."
   pytest tests/test_integration.py -v

   echo "Running performance tests..."
   pytest tests/test_performance.py -v
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ‰€æœ‰APIæµ‹è¯•é€šè¿‡
- é›†æˆæµ‹è¯•è¦†ç›–ä¸»è¦æµç¨‹
- æ€§èƒ½æµ‹è¯•æ»¡è¶³è¦æ±‚

### 13.2 AIæµç¨‹é›†æˆæµ‹è¯• (ç¬¬2å¤©)

**ç›®æ ‡**: æµ‹è¯•å®Œæ•´çš„AIå¤„ç†æµç¨‹ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿä¸»å¯¼ï¼Œåç«¯å›¢é˜Ÿé…åˆ

**å…·ä½“æ­¥éª¤**:

1. **æ–‡æ¡£å¤„ç†æµç¨‹æµ‹è¯•**
   ```python
   def test_document_processing_pipeline():
       """æµ‹è¯•æ–‡æ¡£å¤„ç†å®Œæ•´æµç¨‹"""
       # ä¸Šä¼ æ–‡æ¡£ -> IDPè§£æ -> è¯­ä¹‰åˆ‡åˆ† -> å‘é‡åŒ– -> ç´¢å¼•
       # éªŒè¯æ¯ä¸ªæ­¥éª¤çš„è¾“å‡º
       pass

   def test_knowledge_retrieval():
       """æµ‹è¯•çŸ¥è¯†æ£€ç´¢æµç¨‹"""
       # ç”¨æˆ·æŸ¥è¯¢ -> æ··åˆæ£€ç´¢ -> é‡æ’åº -> è¿”å›ç»“æœ
       # éªŒè¯æ£€ç´¢è´¨é‡å’Œæ€§èƒ½
       pass

   def test_agent_conversation():
       """æµ‹è¯•Agentå¯¹è¯æµç¨‹"""
       # ç”¨æˆ·é—®é¢˜ -> åˆ†æ -> æ£€ç´¢ -> ç”Ÿæˆæ–¹æ¡ˆ
       # éªŒè¯å¤šè½®å¯¹è¯é€»è¾‘
       pass
   ```

2. **æ€§èƒ½åŸºå‡†æµ‹è¯•**
   ```python
   def test_performance_benchmarks():
       """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
       # æ–‡æ¡£è§£æé€Ÿåº¦: <30ç§’/æ–‡æ¡£
       # æ£€ç´¢å“åº”æ—¶é—´: <500ms
       # LLMç”Ÿæˆæ—¶é—´: <3ç§’
       # å¹¶å‘å¤„ç†èƒ½åŠ›: 10ä¸ªå¹¶å‘è¯·æ±‚
       pass
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ‰€æœ‰AIæµç¨‹æµ‹è¯•é€šè¿‡
- æ€§èƒ½æŒ‡æ ‡è¾¾åˆ°è¦æ±‚
- é”™è¯¯å¤„ç†æœºåˆ¶å®Œå–„

### 13.3 é”™è¯¯å¤„ç†å’Œæ—¥å¿— (ç¬¬3å¤©)

**ç›®æ ‡**: å®Œå–„é”™è¯¯å¤„ç†å’Œæ—¥å¿—ç³»ç»Ÿã€‚

**è´Ÿè´£äºº**: åç«¯å›¢é˜Ÿä¸»å¯¼

**å…·ä½“æ­¥éª¤**:

1. **å…¨å±€é”™è¯¯å¤„ç†** (`app/errors.py`)
   ```python
   from flask import jsonify
   from app import db

   def register_error_handlers(app):
       @app.errorhandler(400)
       def bad_request(error):
           return jsonify({
               'code': 400,
               'status': 'error',
               'error': {
                   'type': 'INVALID_REQUEST',
                   'message': 'è¯·æ±‚å‚æ•°é”™è¯¯'
               }
           }), 400

       @app.errorhandler(401)
       def unauthorized(error):
           return jsonify({
               'code': 401,
               'status': 'error',
               'error': {
                   'type': 'UNAUTHORIZED',
                   'message': 'æœªæˆæƒè®¿é—®'
               }
           }), 401

       @app.errorhandler(500)
       def internal_error(error):
           db.session.rollback()
           return jsonify({
               'code': 500,
               'status': 'error',
               'error': {
                   'type': 'INTERNAL_ERROR',
                   'message': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯'
               }
           }), 500
   ```

2. **æ—¥å¿—é…ç½®** (`app/logging_config.py`)
   ```python
   import logging
   from logging.handlers import RotatingFileHandler
   import os

   def setup_logging(app):
       if not app.debug:
           if not os.path.exists('logs'):
               os.mkdir('logs')

           file_handler = RotatingFileHandler(
               'logs/ip_expert.log',
               maxBytes=10240000,
               backupCount=10
           )
           file_handler.setFormatter(logging.Formatter(
               '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'
           ))
           file_handler.setLevel(logging.INFO)
           app.logger.addHandler(file_handler)

           app.logger.setLevel(logging.INFO)
           app.logger.info('IP Expert startup')
   ```

**éªŒæ”¶æ ‡å‡†**:
- é”™è¯¯å¤„ç†è¦†ç›–æ‰€æœ‰åœºæ™¯
- æ—¥å¿—è®°å½•è¯¦ç»†å‡†ç¡®
- é”™è¯¯å“åº”æ ¼å¼ç»Ÿä¸€

### 13.4 éƒ¨ç½²å‡†å¤‡ (ç¬¬4å¤©)

**ç›®æ ‡**: å‡†å¤‡ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²é…ç½®ã€‚

**è´Ÿè´£äºº**: åç«¯å›¢é˜Ÿä¸»å¯¼ï¼Œæ¨¡å‹å›¢é˜Ÿé…åˆ

**å…·ä½“æ­¥éª¤**:

1. **DockeråŒ–** (`Dockerfile`)
   ```dockerfile
   FROM python:3.9-slim

   WORKDIR /app

   COPY requirements.txt .
   RUN pip install -r requirements.txt

   COPY . .

   EXPOSE 5000

   CMD ["gunicorn", "--bind", "0.0.0.0:5000", "app:create_app()"]
   ```

2. **ç”Ÿäº§é…ç½®** (`docker-compose.prod.yml`)
   ```yaml
   version: '3.8'
   services:
     backend:
       build: .
       ports:
         - "5000:5000"
       environment:
         - FLASK_ENV=production
         - DATABASE_URL=mysql+pymysql://user:pass@mysql/ip_expert
       depends_on:
         - mysql
         - redis

     worker:
       build: .
       command: python worker.py
       depends_on:
         - redis
         - mysql
   ```

**éªŒæ”¶æ ‡å‡†**:
- Dockeré•œåƒæ„å»ºæˆåŠŸ
- ç”Ÿäº§ç¯å¢ƒé…ç½®å®Œæ•´
- æœåŠ¡å¯åŠ¨æ­£å¸¸

---

## ğŸ¯ ä»»åŠ¡14: (è¿›é˜¶)å°æ¨¡å‹å¾®è°ƒ

### 14.1 æ•°æ®å‡†å¤‡ (ç¬¬1-2å¤©)

**ç›®æ ‡**: å‡†å¤‡ç”¨äºå¾®è°ƒçš„è®­ç»ƒæ•°æ®é›†ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿ

**å…·ä½“æ­¥éª¤**:

1. **æ”¶é›†é—®é¢˜åˆ†ç±»æ•°æ®**
   ```python
   # æ„å»ºé—®é¢˜åˆ†ç±»æ•°æ®é›†
   classification_data = [
       {"text": "OSPFé‚»å±…å»ºç«‹å¤±è´¥", "label": "OSPF"},
       {"text": "BGPè·¯ç”±ä¸é€š", "label": "BGP"},
       {"text": "MPLS VPNé…ç½®é—®é¢˜", "label": "MPLS"},
       # ... æ›´å¤šæ•°æ®
   ]
   ```

2. **æ•°æ®æ¸…æ´—å’Œæ ‡æ³¨**
   - å»é‡å’Œæ ¼å¼ç»Ÿä¸€
   - äººå·¥æ ‡æ³¨å’Œæ ¡éªŒ
   - æ•°æ®é›†åˆ’åˆ†(è®­ç»ƒ/éªŒè¯/æµ‹è¯•)

**éªŒæ”¶æ ‡å‡†**:
- æ”¶é›†åˆ°1000+æ ‡æ³¨æ ·æœ¬
- æ•°æ®è´¨é‡é«˜ï¼Œæ ‡æ³¨å‡†ç¡®
- æ•°æ®é›†åˆ’åˆ†åˆç†

### 14.2 æ¨¡å‹å¾®è°ƒ (ç¬¬3-5å¤©)

**ç›®æ ‡**: å¾®è°ƒBERTæ¨¡å‹ç”¨äºé—®é¢˜æ„å›¾åˆ†ç±»ã€‚

**è´Ÿè´£äºº**: æ¨¡å‹å›¢é˜Ÿ

**å…·ä½“æ­¥éª¤**:

1. **é€‰æ‹©åŸºç¡€æ¨¡å‹**
   ```python
   from transformers import AutoTokenizer, AutoModelForSequenceClassification

   model_name = "bert-base-chinese"
   tokenizer = AutoTokenizer.from_pretrained(model_name)
   model = AutoModelForSequenceClassification.from_pretrained(
       model_name,
       num_labels=len(categories)
   )
   ```

2. **è®­ç»ƒè„šæœ¬**
   ```python
   from transformers import Trainer, TrainingArguments

   training_args = TrainingArguments(
       output_dir="./results",
       num_train_epochs=3,
       per_device_train_batch_size=16,
       per_device_eval_batch_size=64,
       warmup_steps=500,
       weight_decay=0.01,
       logging_dir="./logs",
   )

   trainer = Trainer(
       model=model,
       args=training_args,
       train_dataset=train_dataset,
       eval_dataset=eval_dataset,
   )

   trainer.train()
   ```

**éªŒæ”¶æ ‡å‡†**:
- æ¨¡å‹è®­ç»ƒå®Œæˆ
- éªŒè¯é›†å‡†ç¡®ç‡>85%
- æ¨¡å‹å¯ä»¥æ­£å¸¸æ¨ç†

---

## ğŸ“Š è¿›åº¦è·Ÿè¸ªä¸è´¨é‡æ§åˆ¶

### æ¯æ—¥ç«™ä¼š
- **æ—¶é—´**: æ¯å¤©ä¸Šåˆ9:00
- **å†…å®¹**: æ±‡æŠ¥æ˜¨æ—¥å®Œæˆæƒ…å†µã€ä»Šæ—¥è®¡åˆ’ã€é‡åˆ°çš„é—®é¢˜
- **å‚ä¸äºº**: åç«¯+æ¨¡å‹å›¢é˜Ÿ5äºº + é¡¹ç›®è´Ÿè´£äºº

### å›¢é˜Ÿåä½œåˆ†å·¥
#### åç«¯å›¢é˜Ÿ (3äºº) ä¸»è¦è´Ÿè´£:
- Flaské¡¹ç›®æ¶æ„å’ŒAPIå¼€å‘
- æ•°æ®åº“è®¾è®¡å’Œæ¨¡å‹å®šä¹‰
- å¼‚æ­¥ä»»åŠ¡ç³»ç»Ÿå’Œé˜Ÿåˆ—ç®¡ç†
- ç”¨æˆ·è®¤è¯å’Œæƒé™ç®¡ç†
- æ•°æ®çœ‹æ¿å’Œç»Ÿè®¡API
- ç³»ç»Ÿéƒ¨ç½²å’Œè¿ç»´

#### æ¨¡å‹å›¢é˜Ÿ (2äºº) ä¸»è¦è´Ÿè´£:
- æ•°æ®æ”¶é›†å’ŒIDPé›†æˆ
- å‘é‡æ•°æ®åº“æ„å»ºå’Œé…ç½®
- æ··åˆæ£€ç´¢ç®—æ³•å®ç°
- æç¤ºè¯å·¥ç¨‹å’Œä¼˜åŒ–
- langgraph Agentæµç¨‹æ„å»º
- æ¨¡å‹æ¥å£é›†æˆå’Œæ€§èƒ½ä¼˜åŒ–

#### å…±åŒåä½œåŒºåŸŸ:
- `app/services/` ç›®å½•ä¸‹çš„AIæœåŠ¡æ¨¡å—
- å¼‚æ­¥ä»»åŠ¡çš„AIå¤„ç†é€»è¾‘
- APIæ¥å£çš„AIåŠŸèƒ½é›†æˆ
- ç³»ç»Ÿé›†æˆæµ‹è¯•å’Œè°ƒè¯•

### ä»£ç å®¡æŸ¥
- **é¢‘ç‡**: æ¯ä¸ªåŠŸèƒ½æ¨¡å—å®Œæˆå
- **æ ‡å‡†**: ä»£ç è§„èŒƒã€å®‰å…¨æ€§ã€æ€§èƒ½ã€æµ‹è¯•è¦†ç›–ç‡
- **å·¥å…·**: Git Pull Request + Code Review

### è´¨é‡æ£€æŸ¥ç‚¹
1. **ç¬¬ä¸€å‘¨æœ«**: åŸºç¡€æ¶æ„ã€è®¤è¯ç³»ç»Ÿã€æ•°æ®æ”¶é›†å’ŒIDPé›†æˆéªŒæ”¶
2. **ç¬¬äºŒå‘¨æœ«**: æ ¸å¿ƒAPIåŠŸèƒ½ã€å‘é‡æ•°æ®åº“ã€æ··åˆæ£€ç´¢ç®—æ³•éªŒæ”¶
3. **ç¬¬ä¸‰å‘¨æœ«**: Agentæµç¨‹ã€æ¨¡å‹é›†æˆã€å®Œæ•´ç³»ç»Ÿé›†æˆæµ‹è¯•éªŒæ”¶

### äº¤ä»˜ç‰©æ¸…å•
#### åç«¯äº¤ä»˜ç‰©:
- [ ] Flaskåç«¯åº”ç”¨ä»£ç 
- [ ] æ•°æ®åº“è®¾è®¡æ–‡æ¡£å’Œè¿ç§»è„šæœ¬
- [ ] APIæ–‡æ¡£å’Œæµ‹è¯•ç”¨ä¾‹
- [ ] å¼‚æ­¥ä»»åŠ¡å¤„ç†ç³»ç»Ÿ
- [ ] Dockeréƒ¨ç½²é…ç½®
- [ ] ç³»ç»Ÿç›‘æ§å’Œæ—¥å¿—é…ç½®

#### AIæ¨¡å‹äº¤ä»˜ç‰©:
- [ ] å¤„ç†åçš„çŸ¥è¯†åº“æ•°æ®é›†
- [ ] Weaviateå‘é‡æ•°æ®åº“(åŒ…å«ç´¢å¼•æ•°æ®)
- [ ] æ··åˆæ£€ç´¢ç®—æ³•ä»£ç 
- [ ] langgraph Agentå·¥ä½œæµ
- [ ] æç¤ºè¯æ¨¡æ¿åº“
- [ ] æ¨¡å‹æ¥å£å°è£…ä»£ç 
- [ ] AIæœåŠ¡é›†æˆæ–‡æ¡£

### é£é™©é¢„æ¡ˆ
#### åç«¯é£é™©:
1. **æ•°æ®åº“æ€§èƒ½é—®é¢˜**: å‡†å¤‡æŸ¥è¯¢ä¼˜åŒ–å’Œç´¢å¼•æ–¹æ¡ˆ
2. **å¼‚æ­¥ä»»åŠ¡å †ç§¯**: å®ç°ä»»åŠ¡ä¼˜å…ˆçº§å’Œé™æµæœºåˆ¶
3. **APIå“åº”è¶…æ—¶**: æ·»åŠ ç¼“å­˜å’Œå¼‚æ­¥å¤„ç†

#### AIæ¨¡å‹é£é™©:
1. **IDPæœåŠ¡è°ƒç”¨å¤±è´¥**: å‡†å¤‡æœ¬åœ°OCRå¤‡é€‰æ–¹æ¡ˆ
2. **å‘é‡åŒ–APIé™æµ**: å®ç°è¯·æ±‚é˜Ÿåˆ—å’Œé‡è¯•æœºåˆ¶
3. **æ¨¡å‹æ•ˆæœä¸ä½³**: å‡†å¤‡å¤šä¸ªPromptç‰ˆæœ¬è¿›è¡ŒA/Bæµ‹è¯•
4. **OLLAMAéƒ¨ç½²é—®é¢˜**: å‡†å¤‡äº‘ç«¯é‡æ’åºAPIå¤‡é€‰æ–¹æ¡ˆ

---

## ğŸ”§ å¼€å‘ç¯å¢ƒé…ç½®

### å¿…éœ€è½¯ä»¶
- Python 3.9+
- MySQL 8.0+
- Redis 7+
- Docker & Docker Compose
- OLLAMA (ç”¨äºæœ¬åœ°æ¨¡å‹éƒ¨ç½²)

### å¼€å‘å·¥å…·
- PyCharm æˆ– VS Code
- Postman (APIæµ‹è¯•)
- MySQL Workbench
- Redis Desktop Manager
- Jupyter Notebook (æ¨¡å‹å®éªŒ)
- Docker Desktop

### ç¯å¢ƒå˜é‡
```bash
export FLASK_APP=app
export FLASK_ENV=development
export SECRET_KEY=dev-secret-key
export DATABASE_URL=mysql+pymysql://root:password@localhost/ip_expert
export REDIS_URL=redis://localhost:6379

# AIæœåŠ¡ç›¸å…³ - Langchainç»Ÿä¸€é›†æˆé…ç½®
export DASHSCOPE_API_KEY=your-dashscope-api-key
export OPENAI_API_KEY=your-dashscope-api-key  # ç”¨äºlangchain_openaiå…¼å®¹æ¥å£
export OPENAI_API_BASE=https://dashscope.aliyuncs.com/compatible-mode/v1

# é˜¿é‡Œäº‘æ–‡æ¡£æ™ºèƒ½æœåŠ¡
export ALIBABA_ACCESS_KEY_ID=your-access-key-id
export ALIBABA_ACCESS_KEY_SECRET=your-access-key-secret

# OLLAMAæœ¬åœ°é‡æ’åºæ¨¡å‹æœåŠ¡
export OLLAMA_BASE_URL=http://localhost:11434
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

### æŠ€æœ¯æ–‡æ¡£
#### åç«¯æŠ€æœ¯:
- [Flaskå®˜æ–¹æ–‡æ¡£](https://flask.palletsprojects.com/)
- [SQLAlchemyæ–‡æ¡£](https://docs.sqlalchemy.org/)
- [RQä»»åŠ¡é˜Ÿåˆ—æ–‡æ¡£](https://python-rq.org/)
- [JWTè®¤è¯æœ€ä½³å®è·µ](https://auth0.com/blog/a-look-at-the-latest-draft-for-jwt-bcp/)

#### AIæŠ€æœ¯:
- [é˜¿é‡Œäº‘æ–‡æ¡£æ™ºèƒ½APIæ–‡æ¡£](https://help.aliyun.com/document_detail/442515.html)
- [Weaviateå®˜æ–¹æ–‡æ¡£](https://weaviate.io/developers/weaviate)
- [LangGraphæ•™ç¨‹](https://langchain-ai.github.io/langgraph/)
- [ç™¾ç‚¼å¤§æ¨¡å‹APIæ–‡æ¡£](https://help.aliyun.com/zh/dashscope/)
- [OLLAMAæ–‡æ¡£](https://ollama.ai/)

### å¼€å‘è§„èŒƒ
- [PEP 8 Pythonä»£ç è§„èŒƒ](https://pep8.org/)
- [RESTful APIè®¾è®¡æŒ‡å—](https://restfulapi.net/)
- [æ•°æ®åº“è®¾è®¡æœ€ä½³å®è·µ](https://www.vertabelo.com/blog/database-design-best-practices/)

### å­¦ä¹ èµ„æº
- RAGæŠ€æœ¯åŸç†ä¸å®è·µ
- å‘é‡æ•°æ®åº“ä½¿ç”¨æŒ‡å—
- æç¤ºè¯å·¥ç¨‹æœ€ä½³å®è·µ
- å¤šè½®å¯¹è¯ç³»ç»Ÿè®¾è®¡
- LangChainå¼€å‘æŒ‡å—

è®°ä½ï¼šä¿æŒä»£ç è´¨é‡ï¼Œæ³¨é‡å®‰å…¨æ€§ï¼ŒåŠ å¼ºå›¢é˜Ÿåä½œï¼ŒåŠæ—¶æ²Ÿé€šé—®é¢˜ï¼ğŸš€

